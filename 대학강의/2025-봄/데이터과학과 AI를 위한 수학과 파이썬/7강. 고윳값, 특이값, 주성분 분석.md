
## 핵심 요약

이 문서는 데이터 과학의 핵심 선형대수 기법인 고윳값 분해, 특이값 분해, 주성분 분석의 원리와 응용을 종합적으로 요약한다. 각 기법은 데이터의 내재적 구조를 파악하고 차원을 축소하여 복잡한 데이터를 효과적으로 분석하는 데 중추적인 역할을 한다.

- **고윳값 분해(Eigenvalue Decomposition):** 정방행렬에 대한 선형 변환의 본질적 특성을 분석하는 기법이다. 행렬이 벡터에 작용할 때 방향은 변하지 않고 크기만 변하는 '고유벡터'와, 이때의 크기 변화율인 '고윳값'을 찾아낸다. 이를 통해 행렬을 대각화하여 복잡한 행렬 연산을 단순화하고 시스템의 동적 특성을 파악할 수 있다.
- **특이값 분해(Singular Value Decomposition, SVD):** 고윳값 분해를 정방행렬이 아닌 일반적인 행렬(m x n)로 확장한 개념이다. 모든 행렬을 왼쪽 특이벡터 행렬(U), 특이값 대각행렬(Σ), 오른쪽 특이벡터 행렬의 전치(Vᵀ)의 곱으로 분해한다. 이는 데이터 압축, 노이즈 제거, 추천 시스템 등 광범위한 분야에서 활용되는 강력한 도구다.
- **주성분 분석(Principal Component Analysis, PCA):** 고차원 데이터의 분산을 가장 잘 설명하는 '주성분' 축을 찾는 대표적인 차원 축소 기법이다. 데이터의 공분산 행렬에 고윳값 분해를 적용하여 데이터의 분산이 가장 큰 방향(고유벡터)을 찾고, 해당 방향으로 데이터를 투영하여 차원을 줄인다. 이를 통해 데이터의 핵심적인 특성을 유지하면서 분석을 용이하게 만든다.

--------------------------------------------------------------------------------

## 1. 고윳값 분해 (Eigenvalue Decomposition)

고윳값 분해는 특정 정방행렬이 가하는 선형 변환의 근본적인 구조를 이해하는 데 사용되는 핵심적인 행렬 분해 방법이다.

### 1.1. 정의와 기본 개념

- **고유벡터(Eigenvector):** 정방행렬 A에 의한 선형 변환 후에도 **방향이 변하지 않고 크기만 변하는** 0이 아닌 벡터(𝑥)를 의미한다.
- **고윳값(Eigenvalue):** 고유벡터의 크기가 변하는 배율(λ)을 나타내는 스칼라 값이다.
- **핵심 방정식:** 고윳값과 고유벡터의 관계는 다음의 방정식으로 정의된다.
    - `A𝑥 = λ𝑥`
    - 이 방정식은 행렬 A가 고유벡터 𝑥에 작용하면, 그 결과는 고유벡터 𝑥에 스칼라 값인 고윳값 λ를 곱한 것과 같음을 의미한다.

### 1.2. 계산 방법

1. **고윳값 계산:** 행렬 A의 고윳값은 다음의 특성방정식(Characteristic Equation)의 해를 통해 구한다.
    - `det(A - λI) = 0` (여기서 I는 단위행렬)
    - 예시: `A = [[2, 4], [5, 3]]`인 경우, 특성방정식은 `λ² - 5λ - 14 = 0`이 되며, 이를 풀면 고윳값 `λ = 7, -2`를 얻는다.
2. **고유벡터 계산:** 계산된 각각의 고윳값(λ)을 `(A - λI)𝑥 = 0` 방정식에 대입하여 해당 고윳값에 대응하는 고유벡터(𝑥)를 구한다. 이 계산에는 가우스 소거법 등이 사용된다.
3. **Python 구현:** `Numpy` 라이브러리의 `linalg.eig()` 함수를 사용하여 고윳값과 고유벡터를 쉽게 계산할 수 있다.

### 1.3. 대각화 (Diagonalization)

- **정의:** 행렬 A의 고유벡터들이 선형 독립(Full Rank)일 경우, 즉 고유벡터 행렬(V)의 역행렬(V⁻¹)이 존재할 때, 행렬 A를 다음과 같이 세 행렬의 곱으로 표현하는 것을 대각화라고 한다.
    - `A = VΛV⁻¹`
        - `V`: 고유벡터를 열벡터로 갖는 행렬 (고유벡터 행렬)
        - `Λ`: 고윳값을 대각원소로 갖는 대각행렬 (고윳값 행렬)
- **의의:** 대각화는 행렬의 본질적인 특성을 고유벡터와 고윳값으로 분해하여 표현하는 것이다. 이를 통해 미분방정식 풀이, 행렬의 거듭제곱 계산 등을 간소화할 수 있다.

### 1.4. 주요 성질

N차원 정방행렬 A에 대해 다음과 같은 성질이 성립한다.

|   |   |   |
|---|---|---|
|성질 번호|내용|수식 표현|
|**1**|N개의 고윳값-고유벡터 쌍을 가진다. (복소수 및 중복 포함)|-|
|**2**|행렬의 대각합(trace)은 모든 고윳값의 합과 같다.|`tr(A) = Σλᵢ`|
|**3**|행렬의 행렬식(determinant)은 모든 고윳값의 곱과 같다.|`det(A) = Πλᵢ`|
|**4**|행렬 A가 대칭행렬이면, N개의 실수 고윳값을 가지며 고유벡터들은 서로 직교한다.|-|
|**5**|행렬 A가 대칭행렬이고 모든 고윳값이 양수이면, 양의 정부호(positive definite)이며 역행렬이 존재한다.|-|
|**6**|행렬 A가 어떤 행렬 X에 대해 `XᵀX` 형태이면, 0 또는 양의 고윳값을 가진다.|-|
|**7**|행렬 X가 풀랭크(full rank)이면, `XᵀX`는 역행렬이 존재한다.|-|

--------------------------------------------------------------------------------

## 2. 특이값 분해 (Singular Value Decomposition, SVD)

특이값 분해는 고윳값 분해를 일반적인 m x n 크기의 행렬로 확장한 강력한 분해 기법이다.

### 2.1. 개념과 구성 요소

- **정의:** 정방행렬이 아닌 임의의 행렬 A를 세 개의 특별한 행렬의 곱으로 분해하는 것이다.
    - `A = UΣVᵀ`
- **구성 요소:**
    - **U (왼쪽 특이벡터 행렬):** N x N 크기의 직교행렬(Orthogonal Matrix)로, 열벡터들이 서로 직교하며 단위벡터이다.
    - **Σ (특이값 행렬):** N x M 크기의 대각행렬로, 대각성분에는 **특이값(Singular Value)**이 큰 순서대로 배열되며 나머지 원소는 모두 0이다. 특이값은 항상 양수이다.
    - **V (오른쪽 특이벡터 행렬):** M x M 크기의 직교행렬로, `Vᵀ`는 V의 전치행렬이다.
- **특징:** 특이값의 개수는 행렬 A의 행과 열의 개수 중 더 작은 값과 같다.
- **Python 구현:** `numpy.linalg.svd()` 또는 `scipy.linalg.svd()` 함수를 사용한다.

--------------------------------------------------------------------------------

## 3. 주성분 분석 (Principal Component Analysis, PCA)

주성분 분석은 고차원 데이터에 내재된 패턴을 찾아 저차원으로 축소하는 대표적인 다변량 데이터 분석 기법이다.

### 3.1. 정의 및 목표

- **목표:** 다차원 공간에 분포된 데이터의 분산(variance)을 가장 잘 설명하는 새로운 축, 즉 **주성분(Principal Component)**을 찾는 것이다.
- **기능:** 데이터의 차원을 축소하여 시각화 및 해석을 용이하게 하고, 변수 간의 높은 상관관계를 제거하여 독립적인 변수로 변환한다.

### 3.2. 공분산 행렬과 고유분해의 역할

PCA의 핵심은 데이터의 공분산 행렬(Covariance Matrix)에 고윳값 분해를 적용하는 것이다.

1. **공분산 행렬 계산:** n개의 개체와 m개의 변수로 구성된 데이터 행렬 X에 대해 공분산 행렬 Σ는 다음과 같이 계산된다.
    - `Σ = (1/n) * XᵀX`
2. **고윳값 분해 적용:**
    - **고유벡터:** 공분산 행렬의 고유벡터는 데이터가 **어떤 방향으로 가장 많이 분산되어 있는지**를 나타낸다. 이 고유벡터들이 바로 주성분 축이 된다.
    - **고윳값:** 공분산 행렬의 고윳값은 데이터를 해당 고유벡터(주성분 축)에 정사영(projection)시켰을 때의 **분산의 크기**를 의미한다.
3. **주성분 결정:** 고윳값이 큰 순서대로 고유벡터를 정렬하면, 데이터의 분산을 가장 많이 설명하는 순서대로 주성분 축(제1주성분, 제2주성분 등)이 결정된다.

### 3.3. 속성 및 응용

- **속성:**
    - 생성된 주성분 축들은 서로 직교(orthogonal)한다.
    - 주성분의 개수는 원래 변수의 개수와 같다.
    - 데이터를 표준화하면, 모든 고윳값의 합은 전체 변수의 개수와 같아진다.
- **응용 분야:**
    - 상관성이 높은 다수의 변수를 서로 독립인 몇 개의 변수로 축소할 때
    - 2차원 또는 3차원 데이터에 대한 1차원 근사 직선을 찾을 때
    - 이미지 데이터의 노이즈를 제거하거나 데이터를 압축할 때
    - 여러 변수들의 가중평균을 계산할 때
- **Python 구현:** `sklearn.decomposition` 서브패키지의 `PCA` 클래스를 사용한다.