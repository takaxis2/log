
## 핵심 요약

본 문서는 데이터 과학 및 인공지능 분야의 핵심 개념인 엔트로피와 관련 정보 이론 측정 지표들을 종합적으로 분석하고 정리한다. 엔트로피는 확률분포가 가진 불확실성 또는 정보량을 수치화한 척도로, 결과의 예측 가능성이 높을수록(확률 차이가 클수록) 엔트로피는 낮아지고, 예측 가능성이 낮을수록(확률이 균등할수록) 높아진다.

이와 연관된 주요 개념으로, 한 변수가 다른 변수를 예측하는 데 얼마나 도움이 되는지를 측정하는 **조건부 엔트로피**, 분류 모델의 예측 성능을 평가하는 오차 함수로 사용되는 **교차 엔트로피**, 그리고 두 변수 간의 상호 의존성을 측정하는 **상호정보량**이 있다. 특히 상호정보량을 연속 변수에 대해 확장한 **최대정보상관계수(MIC)**는 비선형적 관계까지 포착할 수 있는 강력한 도구이다.

이러한 개념들은 의사결정나무의 변수 선택, 분류 모델의 손실 함수 설계, 데이터 특성 간의 관계 분석 등 기계학습의 다양한 영역에서 핵심적인 역할을 수행하며, `scipy`, `sklearn` 등의 파이썬 라이브러리를 통해 효율적으로 구현될 수 있다.

--------------------------------------------------------------------------------

## 1. 엔트로피의 정의와 속성

### 1.1. 기본 개념

엔트로피(Entropy)는 확률분포가 내포하고 있는 정보의 확신도 또는 정보량을 수치로 표현한 것이다. 물리학에서 물질 상태의 분산 정도를 의미하는 것과 유사하게, 정보 이론에서는 불확실성의 정도를 나타낸다.

- **엔트로피와 확률의 관계**:
    - 성공과 실패 같은 특정 결과가 나올 확률의 차이가 클수록(즉, 한쪽 결과가 거의 확실할수록) 엔트로피는 **감소**한다.
    - 확률의 차이가 작을수록(즉, 결과를 예측하기 어려울수록) 엔트로피는 **증가**한다.

### 1.2. 수학적 정의

엔트로피는 확률변수가 이산형인지 연속형인지에 따라 다른 수식으로 정의된다.

|   |   |   |
|---|---|---|
|확률변수 유형|수식|설명|
|**이산확률변수**|𝐻(𝑌) = −∑<sub>𝑘=1</sub><sup>𝐾</sup> 𝑝(𝑦<sub>𝑘</sub>) log<sub>2</sub> 𝑝(𝑦<sub>𝑘</sub>)|`p(y)`는 확률질량함수(PMF)이다.|
|**연속확률변수**|𝐻(𝑌) = −∫<sub>−∞</sub><sup>∞</sup> 𝑝(𝑦) log<sub>2</sub> 𝑝(𝑦) 𝑑𝑦|`p(y)`는 확률밀도함수(PDF)이다.|

### 1.3. 핵심 속성

- **최솟값**: 엔트로피의 최솟값은 **0**이다. 이는 특정 값이 나올 확률이 1인 경우, 즉 불확실성이 전혀 없는 상태에 해당한다.
- **최댓값**: 엔트로피는 모든 확률변수의 값이 동일한 확률을 가질 때 최댓값을 갖는다. 예를 들어, 2<sup>k</sup>개의 값을 갖는 확률변수의 각 확률이 모두 1/2<sup>k</sup>로 동일할 때 엔트로피는 `k`가 된다.

### 1.4. 지니 불순도 (Gini Impurity)

지니 불순도는 엔트로피와 유사하게 확률분포가 어느 한쪽으로 치우쳤는지를 나타내는 척도이다. 로그를 사용하지 않아 계산량이 적기 때문에 엔트로피의 대용으로 많이 사용된다.

- **수식**: 𝐺(𝑌) = ∑<sub>𝑘=1</sub><sup>𝐾</sup> 𝑃(𝑦<sub>𝑘</sub>)(1 − 𝑃(𝑦<sub>𝑘</sub>))

## 2. 조건부 엔트로피와 정보 획득

### 2.1. 결합 엔트로피 (Joint Entropy)

결합 엔트로피는 두 개 이상의 확률변수에 대한 결합확률분포를 사용하여 정의된 엔트로피이다. 이는 두 변수가 함께 가질 수 있는 정보의 총량을 나타낸다.

- **이산확률변수 X, Y의 결합 엔트로피**: 𝐻(𝑋, 𝑌) = −∑<sub>𝑖=1</sub><sup>𝐾<sub>𝑥</sub></sup> ∑<sub>𝑗=1</sub><sup>𝐾<sub>𝑦</sub></sup> 𝑝(𝑥<sub>𝑖</sub>, 𝑦<sub>𝑗</sub>) log<sub>2</sub> 𝑝(𝑥<sub>𝑖</sub>, 𝑦<sub>𝑗</sub>)

### 2.2. 조건부 엔트로피 (Conditional Entropy)

조건부 엔트로피는 확률변수 X의 값을 알고 있을 때, 다른 확률변수 Y의 불확실성이 얼마나 감소하는지를 측정하는 지표이다. 즉, X가 Y를 예측하거나 설명하는 데 얼마나 도움이 되는지를 나타내며 인과관계 분석에 활용된다.

- **정의**: X의 각 값에 대한 Y의 엔트로피를 계산하고, 이를 X의 확률분포로 가중 평균한 값이다.
- **수식 (이산확률변수)**: 𝐻(𝑌|𝑋) = −∑<sub>𝑖=1</sub><sup>𝐾<sub>𝑥</sub></sup> ∑<sub>𝑗=1</sub><sup>𝐾<sub>𝑦</sub></sup> 𝑝(𝑥<sub>𝑖</sub>, 𝑦<sub>𝑗</sub>) log<sub>2</sub> 𝑝(𝑦<sub>𝑗</sub>|𝑥<sub>𝑖</sub>)
- **해석**:
    - `H(Y|X) = 0`이면, X는 Y를 완벽하게 예측할 수 있다.
    - `H(Y|X)` 값이 작을수록 X는 Y에 대한 더 많은 정보를 제공한다.

### 2.3. 활용 사례: 변수 유효성 평가

스팸 메일 분류 문제에서 키워드 존재 여부(X)로 스팸 여부(Y)를 판단할 때, 어떤 키워드가 더 효과적인지 조건부 엔트로피를 통해 평가할 수 있다.

|   |   |   |   |
|---|---|---|---|
|키워드|스팸 여부(Y)와 키워드(X)의 결합분포|조건부 엔트로피 H(Y\|X)|평가|
|**X1**|Y=0일 때 X1=0이 30건, X1=1이 10건<br>Y=1일 때 X1=0이 10건, X1=1이 30건|0.81|X1의 존재 여부가 스팸 예측에 어느 정도 정보를 제공하지만, 불확실성이 여전히 높다.|
|**X2**|Y=0일 때 X2=0이 20건, X2=1이 20건<br>Y=1일 때 X2=0이 40건, X2=1이 0건|**0.69**|X2의 조건부 엔트로피가 더 낮으므로, 스팸 메일을 분류하는 데 **X1보다 더 효과적인 키워드**이다.|

## 3. 교차 엔트로피와 모델 성능 평가

### 3.1. 개념 정의

교차 엔트로피(Cross-Entropy)는 단일 확률변수가 아닌 두 개의 확률분포(p, q)를 인수로 받아, 분류 모델의 성능을 측정하는 데 사용된다. 여기서 `p`는 실제 데이터의 확률분포, `q`는 모델의 예측 확률분포를 나타낸다.

- **수식 (이산확률분포)**: 𝐻(𝑝, 𝑞) = −∑<sub>𝑘=1</sub><sup>𝐾</sup> 𝑝(𝑦<sub>𝑘</sub>) log<sub>2</sub> 𝑞(𝑦<sub>𝑘</sub>)

### 3.2. 오차 함수로서의 역할

교차 엔트로피는 모델의 예측이 틀린 정도를 나타내는 오차 함수(손실 함수)로 기능한다.

- **이진 분류 예시**:
    - 정답(Y)이 1일 때: 예측값(μ)이 1에 가까울수록 교차 엔트로피는 0에 수렴하고, 0에 가까워질수록(예측이 틀릴수록) 값은 무한대로 커진다.
    - 정답(Y)이 0일 때: 예측값(μ)이 0에 가까울수록 교차 엔트로피는 0에 수렴하고, 1에 가까워질수록(예측이 틀릴수록) 값은 무한대로 커진다.
- **로그 손실 (Log-Loss)**: 전체 학습 데이터 N개에 대한 교차 엔트로피의 평균을 의미하며, 분류 모델의 일반적인 손실 함수로 사용된다.
- **카테고리 로그 손실 (Categorical Log-Loss)**: 다중 분류 문제에서 사용되는 교차 엔트로피 손실 함수이다.

### 3.3. 쿨백-라이블러 발산 (Kullback-Leibler Divergence)

쿨백-라이블러 발산(KLD)은 두 확률분포 `p(y)`와 `q(y)`의 모양이 얼마나 다른지를 숫자로 계산한 값으로, 상대 엔트로피(relative entropy)라고도 불린다.

- **수식**: 𝐾𝐿(𝑝 ∥ 𝑞) = 𝐻(𝑝, 𝑞) − 𝐻(𝑝) = ∑<sub>𝑖=1</sub><sup>𝐾</sup> 𝑝(𝑦<sub>𝑖</sub>) log<sub>2</sub>(𝑝(𝑦<sub>𝑖</sub>) / 𝑞(𝑦<sub>𝑖</sub>))
- **특징**:
    - 두 분포가 동일하면 KLD 값은 0이다 (𝐾𝐿(𝑝 ∥ 𝑝) = 0).
    - 거리 개념이 아니므로 대칭적이지 않다 (𝐾𝐿(𝑝 ∥ 𝑞) ≠ 𝐾𝐿(𝑞 ∥ 𝑝)).

## 4. 변수 간 상호 의존성 측정

### 4.1. 상호정보량 (Mutual Information, MI)

상호정보량은 두 확률변수 X와 Y가 공유하는 정보의 양을 측정하는 지표이다. 이는 두 변수의 결합확률분포 `p(x, y)`가 두 변수가 독립이라고 가정했을 때의 확률분포 `p(x)p(y)`와 얼마나 다른지를 KLD로 측정한 값이다.

- **수식**: 𝑀𝐼[𝑋, 𝑌] = 𝐾𝐿(𝑝(𝑥, 𝑦) ∥ 𝑝(𝑥)𝑝(𝑦))
- **엔트로피와의 관계**: 상호정보량은 한 변수의 엔트로피에서 다른 변수를 알았을 때의 조건부 엔트로피를 뺀 값과 같다.
    - 𝑀𝐼[𝑋, 𝑌] = 𝐻(𝑋) − 𝐻[𝑋|𝑌] = 𝐻(𝑌) − 𝐻[𝑌|𝑋]
- **특징**: 두 확률변수가 서로 독립이면 상호정보량은 **0**이다.

### 4.2. 최대정보상관계수 (Maximal Information Coefficient, MIC)

최대정보상관계수(MIC)는 연속확률변수 간의 상호정보량을 추정하기 위한 방법이다. 실제 데이터에서는 확률분포함수를 알 수 없으므로, 데이터를 여러 구간(bin)으로 나누는 히스토그램 방식을 사용한다. MIC는 구간을 나누는 다양한 방법을 시도하여 그중 상호정보량이 가장 큰 값을 찾아 정규화한 값이다.

- **장점**: 피어슨 상관계수가 선형적인 관계만 측정하는 반면, MIC는 **비선형적인 관계**까지 포착할 수 있다.
- **활용**: 의사결정나무 분석에서 연속형 설명변수를 효과적으로 분류(구간화)하는 데 사용된다.

## 5. 파이썬을 이용한 구현

주요 정보 이론 개념들은 다음과 같은 파이썬 라이브러리 및 함수를 통해 계산 및 추정할 수 있다.

|   |   |   |
|---|---|---|
|개념 (Concept)|라이브러리/함수 (Library/Function)|비고 (Notes)|
|**엔트로피 추정** (Entropy Estimation)|`scipy.entropy`|주어진 확률분포로부터 엔트로피를 계산한다.|
|**교차 엔트로피 (로그 손실)**|`sklearn.metrics.log_loss`|분류 모델의 예측 성능을 평가하는 데 사용된다.|
|**상호정보량 (이산형)**|`sklearn.metrics.mutual_info_score`|이산확률변수 간의 상호 의존성을 측정한다.|
|**최대정보상관계수**|`minepy.compute_score`|연속 변수 간의 비선형적 관계를 포함한 의존성을 측정한다.|