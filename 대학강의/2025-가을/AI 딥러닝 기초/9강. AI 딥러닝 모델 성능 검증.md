

### 요약

본 문서는 AI 딥러닝 모델의 성능을 객관적으로 평가하고, 신뢰성을 확보하기 위한 핵심 방법론을 종합적으로 분석한다. 모델 평가는 종속변수 유형에 따라 **범주형(분류) 모델**과 **연속형(회귀) 모델**로 구분되며, 각각에 맞는 고유한 평가지표를 사용해야 한다. 분류 모델은 혼동 행렬(Confusion Matrix)을 기반으로 정확도, 정밀도, 재현율 등을 측정하며, 회귀 모델은 평균 제곱 오차(MSE)와 같은 예측값과 실제값의 차이를 기반으로 평가된다.

모델 개발 시 가장 주의해야 할 문제 중 하나는 **과적합(Overfitting)**이다. 과적합은 모델이 훈련 데이터에만 과도하게 최적화되어 새로운 데이터에 대한 일반화 성능이 저하되는 현상으로, 훈련 손실은 감소하지만 검증 손실이 증가하는 지점을 통해 진단할 수 있다.

이러한 문제들을 해결하고 모델의 효율성을 높이기 위해, 학습이 완료된 모델을 저장하고 재사용하는 기법과 데이터셋을 효율적으로 활용하여 모델의 일반화 성능을 평가하는 **K-겹 교차 검증(K-fold Cross-Validation)** 방법이 필수적으로 요구된다. 본 문서는 이러한 핵심 개념들을 체계적으로 정리하여 신뢰도 높은 AI 모델 구축을 위한 지침을 제공한다.

--------------------------------------------------------------------------------

### 1. 모델 성능 평가 방법론

모델 평가는 데이터 전처리 및 모델 구축 이후, 해당 모델의 예측 성능을 객관적으로 측정하여 사용 가능성을 판단하는 핵심 단계이다. 평가 방법은 모델이 예측하려는 종속변수의 유형에 따라 크게 범주형과 연속형으로 나뉜다.

#### 1.1. 범주형 (분류) 모델 평가 지표

분류 모델의 성능은 실제 범주와 모델이 예측한 범주를 비교하는 **혼동 행렬(Confusion Matrix)**을 기반으로 평가된다. 혼동 행렬은 True Positive(TP), False Negative(FN), False Positive(FP), True Negative(TN)의 네 가지 값으로 구성된다.

|   |   |   |
|---|---|---|
|구분|예측: Positive|예측: Negative|
|**실제: Positive**|True Positive (TP)|False Negative (FN)|
|**실제: Negative**|False Positive (FP)|True Negative (TN)|

이를 바탕으로 한 주요 평가지표는 다음과 같다.

|   |   |   |
|---|---|---|
|평가지표|정의 (계산식)|설명|
|**정확도 (Accuracy)**|`(TP + TN) / (TP + TN + FN + FP)`|전체 데이터 중 정확하게 예측한 비율.|
|**오차 비율 (Error Rate)**|`(FP + FN) / (TP + TN + FN + FP)`|전체 데이터 중 잘못 분류한 비율. (1 - 정확도)|
|**정밀도 (Precision)**|`TP / (TP + FP)`|Positive로 예측한 것 중 실제 Positive인 비율.|
|**재현율 (Recall)**|`TP / (TP + FN)`|실제 Positive인 것 중 모델이 Positive로 예측한 비율. 민감도(Sensitivity), 참 긍정률(TP Rate)이라고도 한다.|
|**특이도 (Specificity)**|`TN / (TN + FP)`|실제 Negative인 것 중 모델이 Negative로 예측한 비율.|
|**F1-Score**|`2 * (Precision * Recall) / (Precision + Recall)`|정밀도와 재현율의 조화 평균으로, 두 지표가 모두 중요할 때 사용된다.|
|**ROC Curve & AUC**|-|ROC Curve는 거짓 긍정률(FP Rate)에 대한 참 긍정률(TP Rate)의 관계를 나타낸 곡선으로, 좌측 상단에 가까울수록 성능이 우수하다. AUC(Area Under the Curve)는 ROC 곡선 아래의 면적으로, 1에 가까울수록 성능이 뛰어나다.|

#### 1.2. 연속형 (회귀) 모델 평가 지표

회귀 모델은 종속변수가 연속형일 때 사용되며, 예측값과 실제값 사이의 오차(차이)를 측정하여 성능을 평가한다.

|                        |                                         |                                                                            |
| ---------------------- | --------------------------------------- | -------------------------------------------------------------------------- |
| 평가지표                   | 정의 (계산식)                                | 설명                                                                         |
| **평균 제곱 오차 (MSE)**     | `(1/n) * Σ(실제값 - 예측값)²`                 | 오차의 제곱에 대한 평균. 오차에 민감하며, 값이 작을수록 좋다.                                       |
| **평균 절대 오차 (MAE)**     | `(1/n) * Σ                              | 실제값 - 예측값                                                                  |
| **평균 제곱근 오차 (RMSE)**   | `√MSE`                                  | MSE에 제곱근을 취한 값으로, 실제 오차와 단위가 같아 해석이 용이하다. 모델의 정밀도를 표현하는 데 적합하다.            |
| **평균 절대 오차율 (MAPE)**   | `(100/n) * Σ                            | (실제값 - 예측값) / 실제값                                                          |
| **평균 제곱 로그 오차 (MSLE)** | `(1/n) * Σ(log(실제값+1) - log(예측값+1))²`   | 실제값이 큰 경우 발생하는 오차의 영향을 줄여준다.                                               |
| **결정계수 (R²)**          | `1 - (SSE / SST)`<br>`=`<br>`SSR / SST` | 모델이 데이터의 변동성을 얼마나 잘 설명하는지를 나타내는 척도. 0과 1 사이의 값을 가지며, 1에 가까울수록 모델의 설명력이 높다. |


- `SST` (총제곱합): 실제값과 평균값의 차이 제곱합
- `SSR` (회귀제곱합): 예측값과 평균값의 차이 제곱합 (모델이 설명하는 변동)
- `SSE` (오차제곱합): 실제값과 예측값의 차이 제곱합 (모델이 설명하지 못하는 변동)

### 2. 과적합 (Overfitting)의 이해와 진단

#### 2.1. 과적합의 정의 및 원인

**과적합(Overfitting)**은 딥러닝 모델이 훈련 데이터에 과도하게 학습되어, 해당 데이터셋 내부에서는 높은 예측 정확도를 보이지만 새로운 데이터(검증 또는 테스트 데이터)에 대해서는 성능이 저하되는 현상을 의미한다.

- **주요 원인:**
    - **복잡한 모델:** 층(Layer)이 너무 많거나, 노드(Node)에 투입되는 변수가 지나치게 복잡한 경우.
    - **부족한 데이터:** 모델이 훈련 데이터의 특성은 물론 잡음(noise)까지 학습하게 되는 경우.
    - **데이터셋 중복:** 훈련 데이터셋과 테스트 데이터셋이 중복되는 경우.

딥러닝은 수많은 변수를 다루기 때문에 항상 과적합의 위험에 주의해야 한다.

#### 2.2. 과적합 진단 방법

과적합을 판단하는 가장 효과적인 방법은 훈련 과정에서 **훈련 손실(Training Loss)**과 **검증 손실(Validation Loss)**을 동시에 모니터링하는 것이다.

1. **데이터 분할:** 전체 데이터를 훈련(Train), 검증(Validation), 평가(Test) 세트로 분할한다. 훈련 및 검증 데이터는 모델 학습 과정에 사용되며, 평가 데이터는 학습이 완료된 모델의 최종 성능을 측정하는 데에만 사용된다.
2. **손실 비교:** 학습을 반복(epoch)하면서 두 손실 값을 비교한다.
    - **정상 학습:** 훈련 손실과 검증 손실이 함께 감소한다.
    - **과적합 발생:** 훈련 손실은 계속 감소하지만, 특정 시점부터 검증 손실이 다시 증가하기 시작한다. 이 지점이 과적합이 발생하는 순간이다.

검증 데이터셋 없이 훈련 데이터셋만으로 모델을 평가하면, 학습이 깊어질수록 정확도가 계속 상승하는 것처럼 보여 과적합을 인지할 수 없다.

### 3. 모델 관리 및 재사용

학습이 완료된 모델을 저장하고 필요할 때 다시 불러와 사용함으로써 반복적인 학습 과정을 생략하고 효율성을 높일 수 있다.

|   |   |   |
|---|---|---|
|기능|함수|설명|
|**모델 저장**|`model.save()`|모델의 구조와 학습된 파라미터(가중치, 절편)를 함께 저장한다. TensorFlow의 SavedModel 포맷이 기본이며, 확장자가 `.h5`일 경우 HDF5 포맷으로 저장된다.|
||`model.save_weights()`|모델의 파라미터(가중치, 절편)만 저장한다.|
|**모델 재사용**|`load_model()`|`save()`로 저장된 모델 전체(구조, 옵티마이저, 파라미터)를 불러온다.|
||`load_weights()`|`save_weights()`로 저장된 파라미터를 불러와 **새로운 모델**에 적용한다. 이 경우, 파라미터를 불러올 모델은 저장했던 모델과 **정확히 동일한 구조**를 가져야 한다.|

### 4. 교차 검증 (Cross-Validation)

#### 4.1. 필요성

실제 프로젝트에서는 모델 성능을 향상시킬 만큼 충분한 양의 데이터를 확보하기 어려운 경우가 많다. 데이터를 훈련셋과 검증셋으로 나누면, 검증셋에 할당된 데이터(예: 20%)는 학습에 사용하지 못하는 비효율이 발생한다. **교차 검증**은 이러한 데이터 부족 문제를 해결하고, 제한된 데이터를 최대한 활용하여 모델의 일반화 성능을 보다 신뢰성 있게 평가하기 위해 고안되었다.

#### 4.2. K-겹 교차 검증 (K-fold Cross-Validation)

K-겹 교차 검증은 가장 널리 사용되는 교차 검증 기법이다.

- **절차:**
    1. 전체 데이터셋을 K개의 동일한 크기의 부분집합(fold)으로 나눈다.
    2. 첫 번째 fold를 검증셋으로 사용하고, 나머지 K-1개의 fold를 학습셋으로 사용하여 모델을 학습하고 평가한다.
    3. 두 번째 fold를 검증셋으로, 나머지를 학습셋으로 사용하여 과정을 반복한다.
    4. 이 과정을 K번 반복하여 각 fold가 한 번씩 검증셋으로 사용되도록 한다.
    5. K개의 평가 결과를 산출한 후, 이들의 평균을 내어 최종 성능 지표로 삼는다.
- **주요 장점:**
    - 모든 데이터를 훈련과 검증에 한 번씩 사용하므로, **데이터의 100%를 학습과 검증에 모두 활용**할 수 있다.
    - 데이터 분할 방식에 따른 성능 변동성을 줄여 모델 평가의 신뢰도를 높인다.