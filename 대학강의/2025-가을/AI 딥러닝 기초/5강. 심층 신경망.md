# 심층 신경망(DNN)의 핵심 원리: 학습, 활성화 함수, 그리고 최적화

## 요약

본 문서는 심층 신경망(Deep Neural Network, DNN)의 핵심 작동 원리를 종합적으로 분석한다. 딥러닝의 근간은 여러 개의 은닉층으로 구성된 심층 신경망을 통해 데이터의 복잡한 패턴을 학습하는 것이다. 이 학습 과정은 크게 두 단계로 구성된다: **순전파(Forward Propagation)**와 **오차 역전파(Backpropagation)**. 순전파 단계에서는 입력 데이터가 가중치와 곱해져 네트워크를 통해 전달되며, **활성화 함수**를 거쳐 최종 출력값을 생성한다. 이후 역전파 단계에서는 예측된 출력값과 실제 정답 간의 오차(Error)를 계산하고, 이 오차를 기반으로 각 가중치를 미세 조정한다.

가중치를 최적화하는 핵심 알고리즘은 **경사 하강법(Gradient Descent)**이다. 경사 하강법은 오차 함수의 기울기를 계산하여 기울기가 가장 낮은 방향으로 가중치를 점진적으로 업데이트함으로써 오차를 최소화한다. 기본 경사 하강법의 한계를 극복하기 위해 **모멘텀(Momentum), RMSProp, 아담(Adam)** 등과 같은 고급 **옵티마이저(Optimizer)**가 개발되었다. 이들은 학습 속도와 안정성을 크게 향상시키며, 특히 모멘텀과 RMSProp의 장점을 결합한 아담(Adam)은 현재 가장 널리 사용되는 최적화 기법이다. 이 모든 과정에서 시그모이드(Sigmoid), 렐루(ReLU)와 같은 활성화 함수는 네트워크에 비선형성을 부여하여 복잡한 문제를 해결할 수 있는 능력을 제공하는 필수적인 요소로 작용한다.

--------------------------------------------------------------------------------

## 1. 딥러닝의 기본 원리: 순전파와 역전파

딥러닝은 은닉층이 2개 이상인 심층 신경망(DNN)을 사용하여 기계가 데이터로부터 가중치(Weight)를 스스로 찾아내도록 자동화하는 학습 방식을 의미한다. 인공신경망은 인간의 뇌 뉴런을 모방하여 여러 계층(Layer)의 노드(Node)를 상호 연결한 구조이며, 이 연결 강도를 나타내는 가중치가 곧 모델의 지식이 된다.

### 1.1. 심층 신경망의 구조와 정보 전달 (순전파)

심층 신경망은 크게 입력층(Input Layer), 은닉층(Hidden Layer), 출력층(Output Layer)으로 구성된다. 정보는 입력층에서 출력층 방향으로 한 단계씩 전달되며, 이 과정을 **순전파(Forward Propagation)**라고 한다.

- **정보 전달 과정**: 각 노드는 이전 계층 노드들로부터 신호를 받아 각각의 가중치와 곱한 후, 그 값들을 모두 합산한다(가중합).
- **활성화 함수 적용**: 계산된 가중합은 활성화 함수를 통과하여 최종 출력 신호로 변환된다. 이 과정을 통해 비선형적인 특성이 모델에 추가된다.
- **선형대수학의 활용**: 이러한 계산 과정은 행렬과 벡터를 이용한 선형대수학 연산으로 효율적으로 처리된다. 예를 들어, 특정 계층의 결과 행렬 `X`는 가중치 행렬 `W`와 입력값 행렬 `I`의 곱(`X = W ∙ I`)으로 표현할 수 있다. 실제 딥러닝 코드는 이러한 행렬 연산을 기반으로 매우 간결하게 구현된다.

### 1.2. 오차 역전파 (Backpropagation)

학습의 핵심은 예측값과 실제 정답 사이의 **오차(Error)**를 줄여나가는 것이다. **오차 역전파**는 출력층에서 계산된 오차를 입력층 방향으로 거꾸로 전파하며 각 연결의 가중치를 조정하는 알고리즘이다.

- **오차의 정의**: 학습 데이터로부터 주어진 정답과 신경망의 출력값 간의 차이.
- **오차의 분배**: 출력층에서 발생한 오차는 이전 계층(은닉층)으로 전달될 때, 각 연결 가중치의 크기에 비례하여 분배된다. 즉, 순전파 시 더 큰 영향을 미쳤던 가중치가 오차에 대한 책임도 더 많이 지게 된다.
- **전 계층으로의 전파**: 이 과정이 모든 계층에 걸쳐 반복되면서, 신경망 전체의 모든 노드가 최종 오차에 얼마나 기여했는지가 계산된다.
- **학습의 핵심**: 딥러닝은 활성화 함수를 이용한 출력값을 목표값과 비교하여 오차를 계산하고, 이 오차를 전 계층에 역전파하면서 연결된 모든 가중치를 보정하는 것이다. 이것이 딥러닝이 지능을 갖추게 되는 핵심 원리이다.

### 1.3. 가중치 업데이트: 경사 하강법 (Gradient Descent)

오차를 기반으로 가중치를 얼마나, 어떻게 조정할지를 결정하는 방법이 **경사 하강법**이다. 신경망은 수많은 가중치의 조합으로 이루어져 있어 대수학적 공식으로 한 번에 최적의 해를 구하거나, 무작위로 값을 대입하는 '무차별 대입' 방식(수억 년이 걸릴 수 있음)은 비현실적이다.

- **경사 하강법의 개념**: 오차를 최소화하는 지점을 찾기 위해, 오차 함수의 기울기(Gradient)가 가장 가파른 방향으로 가중치를 조금씩 이동시키는 반복적인 최적화 방법이다. 이는 마치 산의 가장 낮은 지점을 찾아 한 걸음씩 내려가는 과정에 비유할 수 있다.
- **핵심 원리**:
    - **기울기 계산**: 가중치의 변화에 따라 오차가 얼마나 변하는지를 **미분(Differentiation)**을 통해 계산한다. 이 기울기 값이 오차를 줄이기 위해 가중치를 어느 방향으로 조정해야 할지 알려준다.
    - **가중치 업데이트**: 현재 가중치에서 계산된 기울기의 반대 방향으로 소량 이동시킨다. 기울기가 양수이면 가중치를 감소시키고, 음수이면 증가시킨다.
    - **학습률(Learning Rate)**: 한 번에 얼마나 이동할지를 결정하는 '보폭'에 해당한다. 학습률이 너무 크면 최저점을 지나치는 **오버슈팅(Overshooting)**이 발생할 수 있고, 너무 작으면 학습 속도가 느려진다.
- **과제**: 경사 하강법은 실제 최저점(Global Minimum)이 아닌 지역적 최저점(Local Minimum)에 빠질 위험이 있다. 이를 해결하기 위해 서로 다른 초기 가중치 값으로 여러 번 학습을 시도하는 방법을 사용한다.

최종적으로 은닉층-출력층 간 가중치(`Wjk`) 변화량은 다음 공식의 개념을 따른다. 여기서 `α`는 학습률, `Ek`는 오차, `Ok`와 `Oj`는 각 노드의 출력값을 의미한다.

**∆𝑾𝒋𝒌 = 𝜶 ∙ 𝑬𝒌 ∙ 𝑶𝒌(𝟏 − 𝑶𝒌) ∙ 𝑶𝒋 𝑻**
[[역전파 알고리즘; 오차함수(E)를 가중치 Wjk에 대해 미분하기]]
[[시그모이드 함수 미분 과정]]

## 2. 활성화 함수의 역할과 종류

**활성화 함수(Activation Function)**는 노드가 전달받은 가중합을 특정 기준에 따라 변환하여 출력하는 **비선형 함수**이다. 활성화 함수가 없다면 신경망은 여러 계층을 쌓더라도 결국 하나의 선형 함수와 같아져 복잡한 패턴을 학습할 수 없다.

|   |   |   |
|---|---|---|
|활성화 함수|특징|주요 용도 및 한계|
|**시그모이드 (Sigmoid)**|출력을 0과 1 사이의 값으로 변환한다.|이진 분류 문제에서 확률 값으로 해석하는 데 사용된다. 하지만 층이 깊어지면 기울기가 0에 가까워져 학습이 멈추는 **기울기 소멸 문제(Vanishing Gradient Problem)**를 유발한다.|
|**하이퍼볼릭 탄젠트 (Tanh)**|출력을 -1과 1 사이의 값으로 변환한다. 시그모이드의 출력 평균이 0이 아닌 문제를 일부 해결했다.|시그모이드와 마찬가지로 기울기 소멸 문제가 여전히 존재한다.|
|**렐루 (ReLU)**|입력이 양수이면 그대로 출력하고, 음수이면 0을 출력한다.|학습 속도가 매우 빠르고(하이퍼볼릭 탄젠트 대비 약 6배) 기울기 소멸 문제를 해결하여 은닉층에서 표준적으로 사용된다. 단, 음수 입력에 대해 항상 0을 출력하여 해당 뉴런이 학습 과정에서 비활성화되는 문제가 발생할 수 있다.|
|**리키 렐루 (Leaky ReLU)**|ReLU의 변형으로, 입력이 음수일 때 0이 아닌 매우 작은 값(예: 0.001 * 입력)을 출력한다.|ReLU의 뉴런 비활성화 문제를 해결하여 학습 안정성을 높인다.|
|**소프트맥스 (Softmax)**|다중 클래스 분류 문제의 출력층에서 사용된다. 모든 출력 노드의 값을 정규화하여 총합이 1이 되는 확률 분포로 변환한다.|모델의 최종 예측을 각 클래스에 대한 확률로 표현해준다.|

## 3. 고급 경사 하강법과 옵티마이저

기본적인 경사 하강법은 학습 과정에서 비효율적이거나 불안정한 모습을 보일 수 있다. 이를 개선하기 위해 다양한 변형과 최적화 기법, 즉 **옵티마이저(Optimizer)**가 개발되었다.

### 3.1. 경사 하강법의 주요 변형

- **배치 경사 하강법 (BGD)**: 전체 훈련 데이터셋을 사용하여 한 번의 가중치 업데이트를 수행한다. 안정적이지만 데이터가 클 경우 학습이 매우 오래 걸린다.
- **확률적 경사 하강법 (SGD)**: 데이터셋에서 무작위로 하나의 샘플만 선택하여 가중치를 업데이트한다. 계산이 매우 빠르지만, 업데이트 방향이 불안정하여 정확도가 낮을 수 있다.
- **미니 배치 경사 하강법**: 전체 데이터를 작은 묶음(미니 배치, 보통 32, 64 등 2의 제곱수)으로 나누어 각 미니 배치마다 가중치를 업데이트한다. BGD의 안정성과 SGD의 속도를 절충한 방식으로, 메모리 효율성과 일반화 성능이 우수하여 실제 딥러닝에서 가장 널리 사용된다.

### 3.2. 주요 옵티마이저 분석

옵티마이저는 확률적 경사 하강법의 불안정한 업데이트 문제를 해결하고, 더 빠르고 안정적으로 최적의 가중치를 찾기 위해 학습 속도와 방향을 조절하는 알고리즘이다.

|   |   |   |
|---|---|---|
|옵티마이저|핵심 아이디어|특징|
|**모멘텀 (Momentum)**|이전 업데이트 방향을 일정 비율만큼 현재 업데이트에 반영하여 **관성**을 준다.|지그재그 현상을 줄이고, 학습 방향이 일관될 때 가속하여 더 빠르게 최저점에 도달하도록 돕는다.|
|**네스테로프 모멘텀 (NAG)**|모멘텀의 개선된 버전으로, 관성에 의해 이동할 예상 위치에서 미리 기울기를 계산한다.|불필요한 이동을 줄여 최저점을 지나칠 위험을 감소시키고, 더 정교한 방향으로 업데이트한다.|
|**아다그라드 (Adagrad)**|각 가중치마다 **개별적인 학습률**을 적용한다. 변화가 적었던 가중치는 학습률을 높이고, 변화가 많았던 가중치는 학습률을 낮춘다.|세밀한 조정이 가능하지만, 학습이 진행될수록 학습률이 계속 감소하여 결국 학습이 멈추는 단점이 있다.|
|**알엠에스프롭 (RMSProp)**|아다그라드의 학습률 감소 문제를 해결하기 위해, 최근 기울기 정보에 더 큰 가중치를 부여하여 학습률이 0에 수렴하는 것을 방지한다.|아다그라드보다 안정적인 학습이 가능하다.|
|**아담 (Adam)**|**모멘텀**의 관성 효과와 **RMSProp**의 적응적 학습률 조절 기능을 **결합**한 옵티마이저이다.|빠르고 안정적이며, 다양한 문제에서 좋은 성능을 보여 현재 가장 널리 사용되는 사실상의 표준 옵티마이저이다.|