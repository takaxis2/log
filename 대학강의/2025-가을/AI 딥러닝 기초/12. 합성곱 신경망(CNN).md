## Executive Summary

합성곱 신경망(Convolutional Neural Network, CNN)은 이미지와 같은 그리드 형태의 데이터를 처리하는 데 특화된 딥러닝 모델입니다. 기존의 심층 신경망(DNN)이 이미지를 1차원으로 펼쳐 처리하면서 공간적 구조 정보를 잃는 한계를 극복하기 위해 개발되었습니다. CNN은 이미지의 국소적 영역을 분석하여 계산 효율성을 높이고, 공간적 특성을 보존하여 높은 성능을 발휘합니다.

본 브리핑은 합성곱 신경망의 핵심 원리, 주요 구성 요소, 모델의 내부 동작을 이해하기 위한 시각화 기법, 그리고 사전 훈련된 모델을 활용하는 전이 학습에 대해 종합적으로 분석합니다.

**핵심 통찰:**

1. **핵심 구성 요소:** CNN은 **합성곱층**, **풀링층**, **완전 연결층**으로 구성됩니다. 합성곱층은 필터(커널)를 사용하여 입력 이미지의 특징(Feature)을 추출하고, 풀링층은 추출된 특성 맵의 크기를 줄여 연산량을 감소시키고 핵심 특징을 강조합니다. 마지막으로 완전 연결층이 추출된 특징을 바탕으로 이미지를 분류합니다.
2. **설명 가능한 AI (XAI):** CNN은 '블랙박스' 모델로 인식되지만, **가중치 시각화**와 **특성 맵 시각화**를 통해 내부 동작을 해석할 수 있습니다. 가중치 시각화는 필터가 어떤 시각적 패턴(예: 모서리, 질감)을 학습했는지 보여주며, 특성 맵 시각화는 입력 이미지의 어느 부분이 특정 필터에 의해 활성화되는지를 보여주어 모델의 판단 근거에 대한 신뢰성을 높입니다.
3. **전이 학습 (Transfer Learning):** 대규모 데이터셋(예: ImageNet)으로 사전 훈련된 모델(예: ResNet50)의 가중치를 가져와 새로운 과제에 적용하는 기법입니다. 이는 대량의 데이터가 부족한 상황에서도 높은 성능의 모델을 효율적으로 구축할 수 있게 하는 강력한 방법론입니다. 특히, 사전 훈련된 모델의 합성곱층을 '특성 추출기'로 활용하고 분류기 부분만 새로 훈련시키는 방식이 널리 사용됩니다.

## 1. 합성곱 신경망(CNN)의 기본 원리

### 1.1. CNN의 필요성: 심층 신경망(DNN)의 한계 극복

기존의 심층 신경망(DNN)은 이미지 분석 시 2차원 배열의 이미지를 1차원으로 펼쳐서(Flattening) 처리합니다. 이 과정에서 픽셀 간의 공간적 구조 정보가 무시되는 근본적인 문제가 발생합니다. 또한, 이미지 전체 픽셀에 대해 가중치를 계산하므로 계산 과정이 복잡하고 막대한 컴퓨팅 자원(CPU, GPU, 메모리)을 요구합니다.

합성곱 신경망(CNN)은 이러한 문제를 해결하기 위해 등장했습니다. CNN은 이미지 전체를 한 번에 계산하는 대신, 필터(커널)를 사용하여 이미지의 국소적 부분(Local Region)을 순차적으로 계산합니다. 이를 통해 다음과 같은 이점을 얻습니다.

- **공간적 구조 유지:** 이미지의 2차원 형태를 유지하며 특징을 추출하여 공간 정보를 보존합니다.
- **자원 및 시간 절약:** 국소적 연산을 통해 계산량을 줄여 자원 소모와 처리 시간을 크게 절약합니다.

### 1.2. CNN의 기본 구조

CNN은 주로 5개의 계층으로 구성되어 이미지 데이터 처리, 특히 분류 문제에 최적화된 구조를 가집니다.

1. **입력층 (Input Layer):** 이미지 데이터를 최초로 입력받는 계층입니다. 이미지는 높이(Height), 너비(Width), 채널(Channel)의 3차원 데이터로 처리됩니다. (예: 흑백 이미지는 채널 1, 컬러(RGB) 이미지는 채널 3)
2. **합성곱층 (Convolutional Layer):** CNN의 핵심으로, 필터를 사용하여 입력 데이터로부터 특징을 추출하고 **특성 맵(Feature Map)**을 생성합니다.
3. **풀링층 (Pooling Layer):** 합성곱층에서 생성된 특성 맵의 크기(가로, 세로)를 줄여 연산량을 감소시키고, 중요한 특징을 더욱 두드러지게 만듭니다.
4. **완전 연결층 (Fully Connected Layer):** 추출된 다차원 특성 맵을 1차원 벡터로 펼친 후, 이를 입력받아 최종 분류를 수행합니다.
5. **출력층 (Output Layer):** 완전 연결층의 결과를 소프트맥스(Softmax)와 같은 활성화 함수를 통해 최종 분류 확률 값으로 출력합니다.

전체적인 흐름은 **[합성곱층 → 풀링층]** 구조를 여러 번 반복하여 이미지의 저수준 특징(선, 모서리)부터 고수준 특징(객체의 일부)까지 점진적으로 추출하고, 이를 완전 연결층에서 종합하여 최종 결론을 도출하는 방식입니다.

## 2. CNN의 핵심 구성 요소 상세 분석

### 2.1. 합성곱층(Convolutional Layer): 특징 추출

합성곱층은 입력 데이터에 **필터(Filter) 또는 커널(Kernel)**이라는 작은 행렬을 적용하여 특징을 감지합니다.

- **필터/커널:** 입력에 곱해지는 가중치(W)의 집합으로, 특정 패턴(수직선, 수평선, 특정 색상 조합 등)을 감지하도록 학습됩니다. 일반적으로 (3, 3) 또는 (5, 5) 크기가 사용됩니다.
- **합성곱 연산:** 필터가 입력 이미지 위를 지정된 간격(**스트라이드, Stride**)으로 이동하며, 각 위치에서 필터와 이미지 영역의 대응되는 원소끼리 곱한 후 모두 더하여 하나의 출력 값을 생성합니다.
- **특성 맵 (Feature Map):** 필터 하나가 입력 이미지 전체를 순회하며 계산을 마치면 하나의 특성 맵이 생성됩니다. 이는 해당 필터가 감지한 특징이 이미지의 어느 위치에서 활성화되는지를 나타냅니다.
- **다중 필터 및 3차원 합성곱:**
    - 하나의 합성곱층에서 여러 개의 필터를 사용하면, 각 필터가 서로 다른 특징을 추출하여 여러 개의 특성 맵이 생성됩니다. 이 특성 맵들은 쌓여서 3차원 배열을 이룹니다.
    - 컬러 이미지(예: 4x4x3)와 같이 깊이(채널)가 있는 입력의 경우, 필터 또한 동일한 깊이(예: 3x3x3)를 가져야 합니다. 연산 결과는 차원과 관계없이 항상 하나의 값으로 출력됩니다.

### 2.2. 패딩(Padding): 정보 손실 방지

합성곱 연산을 수행하면 출력 특성 맵의 크기는 입력보다 작아집니다. 이 과정에서 이미지의 외곽(모서리)에 위치한 정보가 중앙부 정보보다 연산에 적게 참여하여 소실될 가능성이 있습니다. **패딩(Padding)**은 이러한 문제를 해결하기 위해 입력 배열의 주위를 특정 값(주로 0)으로 채우는 기법입니다.

- **패딩의 목적:**
    1. **정보 손실 방지:** 모서리 픽셀이 합성곱 연산에 더 많이 참여하도록 하여 외곽 정보의 중요도를 높입니다.
    2. **크기 유지:** 출력 특성 맵의 크기를 입력과 동일하게 유지할 수 있습니다. (이를 **'세임 패딩(same padding)'**이라 함)
- **종류:**
    - **세임 패딩 (Same Padding):** 출력 크기가 입력 크기와 같도록 패딩을 추가합니다.
    - **밸리드 패딩 (Valid Padding):** 패딩을 사용하지 않고 순수한 입력 배열에서만 합성곱을 수행합니다. (Keras의 기본값)

### 2.3. 풀링층(Pooling Layer): 차원 축소 및 연산량 감소

**풀링(Pooling)**은 합성곱층에서 생성된 특성 맵의 가로, 세로 크기를 줄여 연산량을 감소시키고, 미세한 위치 변화에도 모델이 강건하게 반응하도록 돕는 역할을 합니다.

- **주요 특징:**
    - 가중치가 없어 학습 대상이 아닙니다.
    - 특성 맵의 채널(깊이) 수는 변경하지 않고 가로, 세로 크기만 줄입니다.
- **종류:**
    - **최대 풀링 (Max Pooling):** 대상 영역에서 가장 큰 값을 추출합니다. 특징을 가장 잘 나타내는 값을 보존하므로 일반적으로 가장 많이 사용됩니다. 평균 풀링에 비해 중요한 특징이 희미해지는 것을 방지합니다.
    - **평균 풀링 (Average Pooling):** 대상 영역의 모든 값의 평균을 계산합니다.

### 2.4. 완전 연결층 및 출력층: 분류

반복적인 합성곱과 풀링을 거쳐 추출된 최종 특성 맵은 분류를 위해 **완전 연결층(Fully Connected Layer)**으로 전달됩니다.

- **플래튼 (Flatten):** 3차원 형태의 특성 맵을 1차원 벡터로 펼치는 과정입니다.
- **분류:** 1차원 벡터는 일반적인 신경망의 은닉층(Dense Layer)을 거친 후, 최종 **출력층**으로 전달됩니다. 출력층에서는 소프트맥스(Softmax) 활성화 함수를 통해 각 클래스(레이블)에 속할 확률을 계산하며, 가장 높은 확률을 갖는 클래스가 최종 예측 결과로 선정됩니다.

## 3. 설명 가능한 AI(XAI): CNN 모델의 시각화

CNN은 성능이 뛰어나지만 내부 동작 원리를 직관적으로 파악하기 어려운 '블랙박스' 모델의 특성을 가집니다. **설명 가능한 CNN(Explainable CNN)**은 모델의 처리 과정을 시각화하여 결과에 대한 신뢰성을 확보하는 기술입니다.

### 3.1. 가중치 시각화: 학습된 패턴 분석

합성곱층의 필터(가중치)를 시각화하면, 모델이 이미지의 어떤 시각적 패턴을 학습했는지 확인할 수 있습니다.

- **훈련 전 가중치:** 균등 분포에서 무작위로 초기화되어 특별한 패턴이 없는 밋밋한 형태로 나타납니다.
- **훈련 후 가중치:** 학습이 진행되면서 필터들은 특정 패턴(예: 수직선, 대각선, 곡선, 특정 질감 등)을 감지하는 형태로 발전합니다. 신경망의 앞부분에 위치한 층은 단순한 시각적 정보를, 뒷부분으로 갈수록 더 추상적이고 복잡한 정보를 학습합니다.

### 3.2. 특성 맵 시각화: 입력 데이터 활성화 분석

특성 맵(활성화 맵) 시각화는 특정 입력 이미지가 주어졌을 때, 각 필터가 이미지의 어느 부분에서 강하게 활성화되는지를 보여줍니다.

- **분석 방법:** Keras의 **함수형 API(Functional API)**를 사용하여 기존 모델의 입력과 특정 중간층(예: 첫 번째 합성곱층)의 출력을 연결하는 새로운 모델을 만듭니다. 이 모델에 이미지를 입력하면 해당 층의 특성 맵을 직접 얻을 수 있습니다.
- **해석:** 시각화된 특성 맵을 통해 특정 필터가 이미지의 어떤 특징(예: 신발의 윤곽, 옷의 무늬)에 반응하는지 파악할 수 있으며, 이는 모델이 올바른 근거로 예측을 수행하고 있는지 판단하는 데 도움을 줍니다.

## 4. 전이 학습(Transfer Learning): 사전 훈련된 모델의 활용

### 4.1. 전이 학습의 개념 및 이점

딥러닝 모델, 특히 CNN을 효과적으로 훈련시키려면 대규모 데이터셋이 필요하지만, 현실적으로 이를 확보하기는 어렵습니다. **전이 학습(Transfer Learning)**은 이러한 문제를 해결하기 위한 강력한 기법입니다.

- **정의:** ImageNet과 같은 아주 큰 데이터셋으로 이미 훈련된 모델, 즉 **사전 훈련된 모델(Pre-trained Model)**의 가중치를 가져와 새로운 과제에 맞게 조정하여 사용하는 것을 의미합니다.
- **주요 이점:**
    - **적은 데이터로 높은 성능:** 비교적 적은 양의 데이터로도 우수한 성능을 달성할 수 있습니다.
    - **학습 시간 단축 및 자원 효율성:** 처음부터 가중치를 학습시킬 필요가 없어 학습 시간을 크게 줄일 수 있습니다.

### 4.2. 특성 추출(Feature Extraction) 기법

전이 학습의 대표적인 방법으로, 사전 훈련된 모델의 구조를 두 부분으로 나누어 활용합니다.

1. **합성곱 기반 (특성 추출기):** 사전 훈련된 모델의 합성곱층과 풀링층 부분. 이미지의 일반적인 특징(선, 질감, 형태 등)을 추출하는 능력을 이미 갖추고 있습니다. 이 부분의 가중치는 **고정(freeze)**하여 더 이상 학습되지 않도록 합니다.
2. **데이터 분류기:** 모델의 마지막에 위치한 완전 연결층 부분. 새로운 과제에 맞게 이 부분만 새로 추가하고 훈련시킵니다.

즉, 사전 훈련된 모델을 강력한 '특성 추출기'로 사용하고, 추출된 특성을 바탕으로 새로운 분류기만 학습시키는 방식입니다.

### 4.3. ResNet50: 대표적인 사전 훈련 모델

**ResNet50**은 Microsoft에서 개발한 50개 층으로 구성된 깊은 합성곱 신경망입니다. ImageNet 데이터셋으로 훈련되었으며, 전이 학습에 널리 사용됩니다. ResNet의 핵심은 **레지듀얼(residual) 학습** 개념을 도입하여 신경망의 깊이가 매우 깊어질 때 발생하는 성능 저하 문제를 효과적으로 해결한 것입니다.