
## Executive Summary

본 문서는 순환 신경망(Recurrent Neural Network, RNN)의 핵심 개념, 구조, 한계점 및 이를 극복하기 위한 발전된 모델인 장단기 메모리(LSTM)와 게이트 순환 유닛(GRU)에 대한 종합적인 분석을 제공한다. 순환 신경망은 텍스트나 시계열 데이터와 같이 순서가 중요한 순차 데이터(Sequential Data) 처리에 특화된 인공 신경망으로, 이전 타임스텝의 정보를 '기억'하여 현재 입력 처리에 활용하는 순환 구조를 특징으로 한다.

핵심적인 내용은 다음과 같다:

- **RNN의 작동 원리:** RNN은 이전 은닉층의 출력(은닉 상태)을 현재 은닉층의 입력으로 재사용함으로써 정보의 연속성을 유지한다. 이 '기억' 메커니즘을 통해 시퀀스 전체의 맥락을 파악할 수 있다.
- **주요 한계점:** RNN은 시퀀스가 길어질수록 초반부의 정보가 희미해지는 '장기 의존성 문제(Long-Term Dependency Problem)'와 학습 과정에서 기울기가 소실되는 '기울기 소멸 문제(Vanishing Gradient Problem)'를 겪는다.
- **진화된 모델: LSTM과 GRU:**
    - **LSTM (Long Short-Term Memory):** '셀 상태'와 세 개의 게이트(망각, 입력, 출력)를 도입하여 정보의 흐름을 정밀하게 제어한다. 이를 통해 장기 의존성 문제를 효과적으로 해결한다.
    - **GRU (Gated Recurrent Unit):** LSTM의 구조를 단순화한 모델로, 망각 게이트와 입력 게이트를 하나의 '업데이트 게이트'로 통합하여 계산 효율성을 높이면서도 LSTM과 유사한 성능을 보인다.
- **성능 비교:** IMDB 영화 리뷰 분류 실습 결과, LSTM과 GRU 모델이 기본적인 SimpleRNN 모델보다 월등히 높은 정확도를 기록했다. 특히 드롭아웃을 적용한 2계층 LSTM 모델과 드롭아웃을 적용한 GRU 모델이 86.5%의 가장 높은 테스트 정확도를 달성하며, 복잡한 순차 데이터 처리에서 발전된 모델의 우수성을 입증했다.

결론적으로, 순차 데이터 처리를 위해서는 기본적인 RNN의 개념을 이해하는 것이 중요하지만, 실제 문제 해결에서는 장기 의존성 문제를 극복한 LSTM 또는 GRU를 사용하는 것이 필수적이다.

--------------------------------------------------------------------------------

## 1. 순환 신경망(RNN)의 개념과 구조

### 가. 순차 데이터(Sequential Data)의 정의

순차 데이터는 데이터의 순서 자체가 중요한 의미를 가지는 데이터를 지칭한다. 이러한 데이터는 순서를 유지한 채로 신경망에 입력되어야 하며, 이전 데이터를 기억하는 기능이 필수적이다.

- **텍스트 데이터:** 단어의 순서가 문장의 의미를 결정한다. (예: "I am a boy")
- **시계열 데이터(Time Series Data):** 일정한 시간 간격으로 기록된 데이터로, 시간의 흐름에 따른 패턴이 중요하다. (예: 주가 변동, 기온 변화)

### 나. RNN의 기본 원리: 기억과 순환 구조

RNN(Recurrent Neural Network)은 시간적 연속성이 있는 데이터를 처리하기 위해 고안된 신경망이다.

- **기억(Memory):** RNN은 현재까지 입력된 데이터를 요약한 정보를 '기억'하는 능력을 갖는다. 새로운 입력이 들어올 때마다 이 기억은 업데이트되며, 최종적으로는 전체 입력 시퀀스를 요약한 정보가 된다.
- **순환(Recurrent):** 이전 은닉층의 출력이 현재 은닉층의 입력으로 다시 전달되는 '반복되는 순환 구조'를 가진다. 이를 통해 이전 타임스텝의 정보를 현재 타임스텝의 계산에 활용한다. 샘플을 처리하는 각 단계를 **타임스텝(timestep)**이라고 부른다.

### 다. RNN의 구조적 요소

RNN의 층은 특별히 **셀(cell)**이라고 부르며, 셀의 출력은 **은닉 상태(hidden state)**라고 칭한다.

- **은닉 상태(Hidden State):** 각 타임스텝에서 셀의 출력으로, 다음 타임스텝의 셀로 전달되어 정보의 연속성을 유지하는 역할을 한다.
- **가중치(Weights):** RNN의 가중치는 모든 타임스텝에서 동일하게 공유된다.
    - **Wxh:** 입력층에서 은닉층으로 전달되는 가중치
    - **Whh:** 이전 타임스텝(`t-1`)의 은닉층에서 현재 타임스텝(`t`)의 은닉층으로 전달되는 가중치
    - **Why:** 은닉층에서 출력층으로 전달되는 가중치

### 라. 학습 과정과 한계

- **BPTT(Back Propagation Through Time):** RNN은 시간의 흐름에 따라 역전파를 수행하는 BPTT를 통해 학습한다. 각 타임스텝에서 발생한 오차를 측정하고, 이를 시간의 역순으로 전파하여 가중치를 업데이트한다.
- **기울기 소멸 문제(Vanishing Gradient Problem):** BPTT 과정에서 시퀀스가 길어질 경우, 오차가 멀리 전파되면서 기울기가 점차 작아져 0에 가까워지는 문제가 발생한다. 이는 가중치가 제대로 업데이트되지 않아 학습이 어려워지는 원인이 된다.
- **장기 의존성 문제(Long-Term Dependency Problem):** 기울기 소멸 문제로 인해, 시퀀스의 초반부에 나타난 중요한 정보가 뒤쪽 타임스텝까지 제대로 전달되지 못하고 희미해지는 현상이다.

이러한 문제를 해결하기 위해 생략된 BPTT(truncated BPTT)를 사용하거나, 근본적으로는 LSTM, GRU와 같은 발전된 구조가 사용된다.

## 2. RNN의 발전: LSTM과 GRU

### 가. 장단기 메모리(LSTM) 신경망

LSTM(Long Short-Term Memory)은 RNN의 장기 의존성 문제를 해결하기 위해 설계된 구조이다. 핵심 아이디어는 정보의 흐름을 정밀하게 제어하는 '게이트' 메커니즘과 장기 기억을 위한 '셀 상태'를 도입한 것이다.

- **셀 상태(Cell State):** 컨베이어 벨트처럼 네트워크 전체를 관통하는 정보의 흐름이다. 게이트에 의해 정보가 추가되거나 제거될 수 있으며, 장기적인 정보를 효과적으로 보존한다.
- **게이트 구조:**
    1. **망각 게이트(Forget Gate):** 과거 정보(이전 셀 상태) 중 어떤 것을 버릴지 결정한다. 시그모이드 함수를 통해 0(완전 제거)에서 1(완전 유지) 사이의 값을 출력하여 이전 셀 상태에 곱한다.
    2. **입력 게이트(Input Gate):** 현재 입력 정보 중 어떤 것을 셀 상태에 저장할지 결정한다. 시그모이드 함수가 업데이트할 값을 정하고, tanh 함수가 새로운 후보 값 벡터를 생성하여 셀 상태를 업데이트한다.
    3. **출력 게이트(Output Gate):** 업데이트된 셀 상태를 바탕으로 어떤 정보를 은닉 상태(출력)로 내보낼지 결정한다.

LSTM은 이러한 게이트 구조를 통해 셀을 통한 중단 없는 기울기 흐름(uninterrupted gradient flow)을 가능하게 하여 기울기 소멸 문제를 완화한다.

### 나. 게이트 순환 유닛(GRU) 신경망

GRU(Gated Recurrent Unit)는 LSTM의 구조를 단순화하여 계산 효율성을 높인 모델이다.

- **구조적 특징:**
    - LSTM의 망각 게이트와 입력 게이트를 하나의 **업데이트 게이트(Update Gate)**로 통합했다. 이 게이트 컨트롤러가 이전 정보를 얼마나 유지하고, 새로운 정보를 얼마나 반영할지를 동시에 제어한다.
    - 별도의 출력 게이트가 없다. 대신 전체 상태 벡터가 매 타임스텝마다 출력되며, **리셋 게이트(Reset Gate)**가 이전 정보 중 어떤 부분을 현재 상태 계산에서 무시할지 결정한다.
- **장점:** LSTM보다 파라미터 수가 적어 구조가 간단하고 계산적으로 효율적이며, 더 빠르게 학습할 수 있다. 많은 경우 LSTM과 유사한 성능을 제공한다.

## 3. 순환 신경망의 응용: 영화 리뷰 분류

### 가. IMDB 데이터셋

- **개요:** 50,000개의 영화 리뷰로 구성된 데이터셋으로, 긍정(1) 리뷰와 부정(0) 리뷰가 각각 50%씩 포함되어 있다.
- **특징:** 리뷰 텍스트가 이미 전처리를 거쳐 각 단어가 고유한 정수로 변환되어 제공된다.

### 나. 텍스트 데이터 전처리

신경망은 텍스트를 직접 처리할 수 없으므로 숫자 데이터로 변환하는 과정이 필요하다.

1. **토큰화(Tokenization):** 문장을 의미 있는 단위인 **토큰(Token)**으로 분리한다. 영어의 경우 일반적으로 공백을 기준으로 단어를 분리하며, 소문자 변환 및 구둣점 제거 등의 전처리를 거친다.
2. **정수 인코딩:** 데이터에 등장하는 고유한 단어(토큰)로 '어휘 사전'을 만들고, 각 단어에 고유한 정수를 할당한다. IMDB 데이터셋에서는 특정 정수가 예약되어 있다.
    - `0`: 패딩 (문장 길이를 맞추기 위함)
    - `1`: 문장 시작
    - `2`: 어휘 사전에 없는 토큰
3. **단어 임베딩(Word Embedding):** 정수 인코딩된 단어를 고정된 크기의 밀집 벡터(dense vector)로 변환하는 기법이다. 이는 단어마다 하나의 원소만 1인 희소 벡터(sparse vector)를 사용하는 원-핫 인코딩의 공간적 낭비를 해결하고, 단어 간의 의미적 관계를 벡터 공간에 표현할 수 있게 해준다.

## 4. 모델 성능 비교 분석

IMDB 영화 리뷰 분류 문제를 대상으로 SimpleRNN, LSTM, GRU 모델의 성능을 비교한 실험 결과는 다음과 같다.

|   |   |   |   |   |   |   |
|---|---|---|---|---|---|---|
|모델 구성|Train Loss|Val Loss|Test Loss|Train Accuracy|Val Accuracy|Test Accuracy|
|RNN (단층)|0.3580|0.5007|0.5106|0.8467|0.7827|0.7798|
|LSTM (단층)|0.3027|0.3426|0.3466|0.8865|0.8605|0.8615|
|LSTM (단층, 드롭아웃)|0.2589|0.3270|0.3356|0.9023|0.8629|0.8622|
|**LSTM (2계층, 드롭아웃)**|0.2702|0.3343|0.3415|0.8992|0.8685|**0.8650**|
|**GRU (단층, 드롭아웃)**|0.2390|0.3339|0.3357|0.9085|0.8642|**0.8650**|
|GRU (2계층, 드롭아웃)|0.2458|0.3500|0.3482|0.9047|0.8653|0.8604|

### 분석 및 결론

- **성능 우위:** LSTM과 GRU 모델은 모든 평가지표에서 기본 SimpleRNN 모델보다 월등히 우수한 성능을 보였다. 이는 장기 의존성 문제를 해결하는 게이트 메커니즘의 효과를 명확히 보여준다.
- **최고 성능:** 드롭아웃을 적용한 2계층 LSTM 모델과 드롭아웃을 적용한 단층 GRU 모델이 86.5%로 가장 높은 테스트 데이터 정확도를 기록했다.
- **효율성:** SimpleRNN은 학습에 많은 시간(약 1시간)이 소요된 반면, LSTM과 GRU는 더 나은 성능을 더 효율적으로 달성했다.
- **시사점:** 복잡한 순차 데이터 분석 시, 문제의 특성과 계산 자원을 고려하여 LSTM 또는 GRU를 선택하는 것이 바람직하다. 다양한 하이퍼파라미터 조정을 통해 성능을 추가로 개선할 수 있다.

## 5. 핵심 용어 정리

- **순차 데이터 (Sequential Data):** 텍스트, 시계열 데이터와 같이 순서에 의미가 있는 데이터.
- **순환 신경망 (RNN):** 순차 데이터를 처리하기 위해 고안된 순환층을 사용한 신경망으로, 셀의 출력인 은닉 상태를 다음 타임스텝에 재사용하는 구조.
- **토큰 (Token):** 텍스트에서 공백 등으로 구분되는 문자열 단위.
- **단어 임베딩 (Word Embedding):** 정수로 변환된 토큰을 작은 크기의 실수 밀집 벡터로 변환하는 기술. 단어 간 관계 표현에 효과적이다.
- **LSTM (Long Short-Term Memory):** 기울기 소멸 문제를 해결하기 위해 망각, 입력, 출력 게이트와 셀 상태를 통해 정보를 관리하는 신경망 구조.
- **GRU (Gated Recurrent Unit):** LSTM보다 간단한 구조로, 망각 게이트와 입력 게이트를 통합하여 계산 효율을 높인 신경망 구조.