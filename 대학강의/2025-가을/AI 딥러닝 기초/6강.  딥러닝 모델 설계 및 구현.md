
### 요약

본 문서는 딥러닝 모델을 구현하는 전반적인 절차와 핵심 개념을 종합적으로 설명한다. 딥러닝 모델 개발은 **데이터 준비, 인공신경망 구조 정의, 데이터 분할, 모델 학습, 모델 평가, 훈련 과정 모니터링**의 6단계로 구성된다.

모델 구조는 `Sequential()` API를 기반으로 `Dense` 레이어를 추가하여 정의하며, 은닉층에는 주로 `relu` 활성화 함수를, 출력층에는 문제 유형에 따라 `sigmoid`(이진 분류) 등의 함수를 사용한다. 데이터는 학습(Train), 검증(Validation), 평가(Test)용으로 분할하여 모델의 일반화 성능을 확보한다.

모델 학습은 `compile()` 메소드로 손실 함수(`loss`), 최적화기(`optimizer`), 평가 지표(`metrics`)를 설정한 후, `fit()` 메소드를 통해 진행된다. 이 과정에서 에포크(epoch)와 미니 배치(mini-batch) 개념이 적용되어 전체 데이터셋을 반복적으로 학습하며 최적의 가중치를 탐색한다. 학습이 완료된 모델은 `evaluate()` 메소드로 최종 성능을 평가하며, 학습 전 과정은 텐서보드(TensorBoard)나 `history` 객체를 통해 시각적으로 모니터링하여 과대/과소적합 등의 문제를 진단하고 개선할 수 있다.

--------------------------------------------------------------------------------

### 1. 딥러닝 모델 구현의 전체 절차

딥러닝 모델을 설계하고 구현하는 과정은 체계적인 단계로 구성된다. 전체 프로세스는 학습 데이터(데이터와 레이블)를 사용하여 인공신경망 모델을 훈련시켜 최적의 가중치를 찾는 **학습 단계**와, 훈련된 모델을 사용하여 새로운 데이터의 결과를 예측하는 **예측 단계**로 나뉜다.

구체적인 구현 절차는 다음과 같은 6개의 핵심 단계로 정의된다.

1. **데이터 준비**: 모델 학습에 필요한 데이터를 로드하고 기본 구조를 파악한다. 예시로 사용된 폐암 수술 환자 데이터는 470명의 환자 정보(샘플)를 16개의 속성(피처)으로 정리한 것이다.
2. **인공신경망 모델 구조 정의**: `Sequential` 모델을 기반으로 입력층, 은닉층, 출력층을 구성한다. 각 층의 노드 수와 활성화 함수를 결정한다.
3. **데이터 분할**: 전체 데이터를 학습(Train), 검증(Validation), 평가(Test) 세트로 분할하여 모델의 객관적인 성능을 측정할 기반을 마련한다.
4. **모델 학습**: `compile` 및 `fit` 명령어를 통해 손실 함수, 최적화 방법 등을 설정하고 실제 학습을 진행한다.
5. **모델 평가**: 학습 과정에서 사용되지 않은 평가(Test) 데이터로 모델의 최종 성능을 측정한다.
6. **훈련 과정 모니터링**: 텐서보드(TensorBoard)나 `history` 객체를 활용하여 학습 중 손실(loss)과 정확도(accuracy)의 변화를 시각화하고 분석한다.

### 2. 인공신경망 모델 구조 정의

#### 모델 생성 및 층 추가

딥러닝 모델의 구조는 Keras의 `Sequential()` 메소드를 선언하며 시작된다. 이는 각 층을 순차적으로 쌓아 올리는 모델을 의미한다.

- `**model = Sequential()**`: 딥러닝 구조를 정의하는 객체를 생성한다.
- `**model.add()**`: 모델에 새로운 층(layer)을 추가한다. 추가된 층의 순서에 따라 입력층, 은닉층, 출력층의 역할을 수행한다.
- **입력층(Input Layer)**: Keras에서는 별도의 입력층을 만들지 않고, 첫 번째 은닉층에 `input_dim` 매개변수를 지정하여 입력 데이터의 차원(피처 수)을 정의한다. 이로써 첫 번째 층이 입력층과 은닉층의 역할을 겸하게 된다.

#### 밀집층(Dense Layer)과 파라미터

각 층의 구체적인 구조는 `Dense()` 메소드로 결정된다. `Dense` 층은 완전 연결층(Fully Connected Layer)이라고도 불리며, 이전 층의 모든 뉴런이 다음 층의 모든 뉴런과 연결된 구조를 의미한다.

- `**Dense(30, input_dim=16, ...)**`:
    - 첫 번째 인자 `30`: 해당 층에 생성할 노드(뉴런)의 수를 의미한다.
    - `input_dim=16`: 모델의 첫 번째 층에서만 사용되며, 입력 데이터가 16개의 속성(피처)으로 구성되어 있음을 명시한다.
- **파라미터(Parameter)**: 모델이 학습 과정에서 최적화하는 가중치(weight)와 편향(bias)을 의미한다. `model.summary()` 메소드로 각 층의 파라미터 수와 전체 파라미터 수를 확인할 수 있다.
    - **계산 예시**: 입력 노드가 16개이고 은닉층 노드가 30개일 때, 첫 번째 Dense 층의 파라미터 수는 `(16 * 30)개의 가중치 + 30개의 편향 = 510개`이다.

#### 활성화 함수(Activation Function)

활성화 함수는 각 노드에서 계산된 가중합을 비선형 값으로 변환하여 모델의 표현력을 높이는 역할을 한다.

- **렐루(ReLU)**: 은닉층에서 주로 사용된다. 입력값이 음수이면 0을, 양수이면 입력값을 그대로 출력한다. 기울기 소멸(vanishing gradient) 문제를 해결하는 데 효과적이다.
- **시그모이드(Sigmoid)**: 주로 이진 분류 문제의 출력층에서 사용된다. 모든 입력값을 0과 1 사이의 확률 값으로 변환한다.

### 3. 데이터 분할

모델의 일반화 성능을 객관적으로 평가하기 위해 데이터를 여러 세트로 분할하는 과정은 필수적이다.

- **분할 종류**:
    - **학습 데이터 (Train Data)**: 모델의 가중치를 학습시키는 데 사용된다. (일반적으로 60-80%)
    - **검증 데이터 (Validation Data)**: 학습 과정 중에 모델의 성능을 검증하며, 과대적합 여부를 판단하고 하이퍼파라미터를 조정하는 데 참고 자료로 사용된다.
    - **평가 데이터 (Test Data)**: 학습이 모두 완료된 후, 모델의 최종 성능을 평가하는 데 단 한 번만 사용된다. 학습 과정에는 전혀 관여하지 않는다.
- **분할 방법**: Scikit-learn 라이브러리의 `train_test_split` 함수를 사용하여 데이터를 학습용과 평가용으로 나눌 수 있으며, `model.fit()` 메소드의 `validation_split` 인자를 통해 학습 데이터의 일부를 검증용으로 추가 분할할 수 있다.

### 4. 모델 학습 프로세스

#### 모델 컴파일:

모델을 훈련시키기 전, 학습 환경을 설정하는 단계이다.

- `**loss**` **(손실 함수)**: 모델의 예측값과 실제 정답 간의 오차를 계산하는 함수. 이 값을 최소화하는 것이 학습의 목표이다.
    - **이진 분류**: `binary_crossentropy` 사용.
    - **다중 분류**: `categorical_crossentropy` 사용.
    - **회귀**: `mse` (평균 제곱 오차) 등 사용.
- `**optimizer**` **(최적화기)**: 계산된 손실을 기반으로 모델의 가중치를 업데이트하는 알고리즘. `adam`이 보편적으로 사용된다.
- `**metrics**` **(평가 지표)**: 훈련 및 테스트 과정을 모니터링할 성능 측정 기준. `accuracy` (정확도)가 주로 사용된다.

|   |   |   |
|---|---|---|
|모델 유형|주요 손실 함수|설명|
|**회귀 (Regression)**|평균 제곱 오차 (MSE)|실제값과 예측값의 차이를 제곱하여 평균 낸 값|
||평균 절대 오차 (MAE)|실제값과 예측값의 차이의 절댓값을 평균 낸 값|
|**분류 (Classification)**|이항 교차 엔트로피 (Binary CE)|두 개의 클래스 중 하나를 예측하는 문제에 사용|
||범주형 교차 엔트로피 (Categorical CE)|세 개 이상의 클래스 중 하나를 예측하는 문제에 사용|

#### 모델 훈련:

실제 데이터를 모델에 주입하여 학습을 진행하는 메소드이다.

- **에포크 (Epoch)**: 전체 학습 데이터셋이 모델을 한 번 통과하여 학습이 완료되는 과정을 '1 에포크'라고 한다. `epochs=5`로 설정하면 전체 데이터셋을 총 5번 반복하여 학습한다.
- **미니 배치 경사 하강법 (Mini-batch Gradient Descent)**: 전체 데이터를 한 번에 처리하는 대신, 작은 묶음(미니 배치)으로 나누어 학습하는 방식. `batch_size=16`은 16개의 샘플마다 기울기를 계산하고 가중치를 업데이트하라는 의미이다.
- **주요 인자**:
    - `X_train`, `y_train`: 학습 데이터와 레이블.
    - `epochs`: 총 학습 반복 횟수.
    - `batch_size`: 한 번에 처리할 샘플 수.
    - `validation_split`: 학습 데이터 중 검증용으로 사용할 비율.
    - `verbose`: 학습 진행 상황 출력 여부 (1은 진행 막대 표시).

### 5. 모델 평가 및 훈련 과정 모니터링

#### 최종 평가:

학습이 완료된 모델의 성능을 평가(Test) 데이터셋으로 최종 검증하는 단계이다. 이 메소드는 모델의 손실 값과 `compile` 단계에서 설정한 평가 지표(예: 정확도)를 반환한다.

- `model.evaluate(X_test, y_test)`: 평가 데이터(`X_test`)에 대한 모델의 예측과 실제 정답(`y_test`)을 비교하여 최종 성능을 계산한다.

#### 훈련 과정 모니터링 도구

학습이 진행되는 동안 모델의 성능 변화를 추적하여 과대적합이나 과소적합과 같은 문제를 진단할 수 있다.

- **텐서보드 (TensorBoard)**: 텐서플로의 시각화 도구. 손실, 정확도 등 각종 지표의 변화를 그래프로 실시간 모니터링할 수 있다.
    1. `tf.keras.callbacks.TensorBoard` 콜백 객체를 생성한다.
    2. `model.fit()` 메소드의 `callbacks` 인자에 생성된 객체를 전달한다.
    3. 터미널 또는 노트북 환경에서 `%tensorboard --logdir <로그_디렉토리>` 명령어로 실행한다.
- **History 객체**: `model.fit()` 메소드는 학습 과정의 모든 지표를 담고 있는 `history` 객체를 반환한다.
    - `history.history` 딕셔너리에는 에포크별 `loss`, `accuracy`, `val_loss`(검증 손실), `val_acc`(검증 정확도) 값이 저장되어 있다.
    - 이 값들을 그래프로 시각화하여 학습 곡선을 분석하고, 훈련 손실은 감소하지만 검증 손실이 증가하는 과대적합 지점을 파악할 수 있다.