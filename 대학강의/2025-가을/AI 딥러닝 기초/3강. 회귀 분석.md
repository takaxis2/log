

## Executive Summary

본 문서는 AI 및 딥러닝의 근간을 이루는 핵심 통계 기법인 회귀 분석을 심층적으로 분석합니다. 주요 내용은 선형 회귀와 로지스틱 회귀의 원리, 그리고 머신러닝의 기본 학습 메커니즘에 중점을 둡니다.

**핵심 통찰:**

1. **회귀 분석의 근본적 역할:** 딥러닝은 수많은 통계적 계산의 집합체이며, 그 가장 기본적인 연산 원리는 선형 회귀와 로지스틱 회귀에서 비롯됩니다. 머신러닝의 본질은 주어진 데이터의 특징을 가장 잘 나타내는 '선'을 찾아내어 미래 값을 예측하거나 데이터를 분류하는 것입니다.
2. **선형 회귀 (Linear Regression):** 독립 변수와 종속 변수 간의 선형 관계를 모델링합니다. '최소제곱법(Least Square Method)'을 사용하여 실제 관측치와 회귀선 간의 오차(잔차) 제곱의 합을 최소화하는 최적의 직선(기울기 `w`와 y절편 `b`로 정의)을 도출합니다. 이는 연속적인 값을 예측하는 데 사용됩니다.
3. **로지스틱 회귀 (Logistic Regression):** 참(1) 또는 거짓(0)과 같은 이진 분류 문제를 해결하기 위해 설계되었습니다. 결과값을 0과 1 사이의 확률로 변환하는 '시그모이드(Sigmoid)' 함수를 사용하여 S자 형태의 결정 경계를 생성합니다.
4. **머신러닝 학습의 본질:** 머신러닝 학습은 예측값과 실제 목표값 사이의 오차(Error)를 기반으로 모델의 매개변수(가중치, 기울기 등)를 반복적으로 미세 조정하는 과정입니다. 이 과정에서 '학습률(Learning Rate)'은 매개변수 업데이트의 폭을 조절하여 모델이 안정적으로 최적의 해에 수렴하도록 돕는 중요한 역할을 합니다.
5. **데이터의 수학적 표현:** 벡터(특징의 나열)와 행렬(벡터의 집합)은 머신러닝에서 데이터를 표현하고 연산하는 기본 단위입니다. 데이터의 각 특징은 다차원 '특징 공간(Feature Space)'의 한 축을 구성하며, 이를 통해 데이터를 기하학적으로 이해하고 분석할 수 있습니다.

--------------------------------------------------------------------------------

## 1. 이해를 위한 기초: 데이터 표현 및 특징 공간

머신러닝 모델이 데이터를 처리하기 위해서는 먼저 데이터를 수학적으로 표현해야 합니다. 이때 벡터와 행렬이 기본 단위로 사용됩니다.

### 벡터와 행렬

- **특징 벡터(Feature Vector):** 데이터 샘플 하나의 여러 특징을 숫자로 나열한 것입니다. 예를 들어, Iris(붓꽃) 데이터의 한 샘플은 [꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비]라는 4개의 특징으로 구성된 4차원 벡터로 표현될 수 있습니다. (예: `[5.1, 3.5, 1.4, 0.2]`)
- **행렬(Matrix):** 여러 개의 특징 벡터를 행(Row)과 열(Column)로 구성하여 모아놓은 것입니다. 데이터셋 전체를 하나의 행렬로 표현할 수 있습니다.
- **행렬 연산:**
    - **행렬곱:** 첫 번째 행렬의 열 개수와 두 번째 행렬의 행 개수가 같아야 가능합니다. 교환법칙(`AB ≠ BA`)은 성립하지 않지만, 분배법칙(`A(B + C) = AB + AC`)과 결합법칙(`A(BC) = (AB)C`)은 성립합니다.
    - **전치행렬(Transpose Matrix, Aᵀ):** 행렬의 행과 열을 바꾼 행렬입니다.
    - **특수 행렬:** 정사각행렬, 대각행렬, 단위행렬, 대칭행렬 등이 있습니다.

### 특징 공간 (Feature Space)

특징 공간은 데이터의 특징들을 축으로 사용하여 데이터를 배치하는 가상의 공간입니다.

- **1차원 특징 공간:** 특징이 하나인 경우, 하나의 축으로 데이터를 표현합니다.
- **2차원 특징 공간:** 특징이 두 개인 경우(예: 몸무게, 키), 2차원 평면에 데이터를 점으로 표현할 수 있습니다.
- **다차원 특징 공간:** 특징이 세 개 이상인 경우입니다. Iris 데이터는 4개의 특징을 가지므로 4차원 특징 공간을 형성합니다. 이 데이터는 1936년 통계학자 피셔(Fisher)가 붓꽃 3종(Setosa, Versicolor, Virginica)을 각각 50송이씩, 총 150개 샘플에 대해 측정한 기록입니다.

## 2. 선형 회귀 분석: 예측의 기본

선형 회귀는 독립 변수(원인)가 종속 변수(결과)에 미치는 영향을 파악하고, 이를 통해 미래의 종속 변수 값을 예측하는 선형 모델을 만드는 기법입니다. "딥러닝을 이해하는 데 중요한 첫걸음"으로 평가됩니다.

### 정의 및 목적

- **목표:** 주어진 데이터의 패턴을 가장 잘 설명하는 하나의 직선을 그리는 것. 이 직선을 통해 데이터에 없는 새로운 입력값에 대한 출력값을 예측할 수 있습니다.
- **단순 선형 회귀:** 하나의 독립 변수(`x`)로 종속 변수(`y`)를 설명합니다.
- **다중 선형 회귀:** 여러 개의 독립 변수(`x1, x2, ...`)로 종속 변수(`y`)를 설명합니다.
- **일차 함수:** 선형 회귀 모델은 일차 함수 `y = wx + b`로 표현됩니다.
    - `x`: 독립 변수 (입력값)
    - `y`: 종속 변수 (출력값)
    - `w`: 가중치 (기울기)
    - `b`: 편향 (y절편)

### 최적의 회귀선 탐색: 최소제곱법

가장 정확한 선, 즉 최적의 `w`와 `b`를 찾는 것이 선형 회귀의 핵심 과제입니다.

- **잔차(Residual):** 실제 관측값과 회귀선이 예측한 값 사이의 차이.
- **최소제곱법(Least Square Method):** 모든 데이터 포인트에 대해 잔차의 제곱의 합이 최소가 되는 직선을 찾는 방법입니다. 잔차는 음수가 될 수 있으므로, 제곱하여 합산함으로써 오차의 총량을 계산합니다.

#### 최소제곱법 공식

- **기울기 (w) 계산:**
- **y절편 (b) 계산:**

#### 적용 예시: 공부 시간과 성적

|   |   |   |
|---|---|---|
|구분|공부 시간 (x)|성적 (y)|
|학생 1|2|81|
|학생 2|4|93|
|학생 3|6|91|
|학생 4|8|97|
|**평균**|**5**|**90.5**|

1. **기울기(w) 계산:** 위 공식을 적용하면 기울기는 `2.3`이 됩니다.
2. **y절편(b) 계산:** `b = 90.5 - (2.3 * 5) = 79`
3. **최종 회귀선:** `y = 2.3x + 79`

이 방정식을 통해 7시간 공부했을 때의 예상 성적 등을 예측할 수 있습니다.

## 3. 머신러닝의 학습 원리

머신러닝은 데이터를 통해 모델의 매개변수를 자동으로 조정하며 최적의 해를 찾아가는 과정입니다. 이는 예측과 분류 문제의 근본적인 해결 방식입니다.

### 매개변수 조정을 통한 학습 메커니즘

학습은 예측값과 실제값의 오차(Error)를 줄여나가는 방향으로 매개변수를 업데이트하는 반복적인 과정입니다.

1. **초기화:** 매개변수(예: 기울기 `A` in `y=Ax`)를 임의의 값으로 초기화합니다. (예: `A = 0.25`)
2. **예측:** 첫 학습 데이터(입력 `x`)를 사용하여 예측값(`y`)을 계산합니다.
3. **오차 계산:** 예측값(`y`)과 실제 목표값(`t`)의 차이인 오차(`E = t - y`)를 계산합니다.
4. **매개변수 업데이트:** 계산된 오차를 기반으로 매개변수를 얼마나 조정할지(`ΔA`) 계산하고, 기존 매개변수를 업데이트합니다. (`새로운 A = 기존 A + ΔA`)

### 학습률(Learning Rate, L)의 역할

만약 오차를 기반으로 한 조정값(`ΔA`)을 그대로 적용하면, 모델이 마지막으로 학습한 데이터에 지나치게 맞춰지는 문제가 발생할 수 있습니다. 이를 방지하기 위해 학습률을 도입합니다.

- **학습률이란?** 매개변수를 업데이트할 때 그 변화의 폭을 조절하는 하이퍼파라미터입니다.
- **조정된 업데이트 공식:** `ΔA = L * (E/x)`
- **역할:**
    - **학습률이 너무 크면:** 최적의 값을 지나쳐버려 모델이 발산할 수 있습니다.
    - **학습률이 너무 작으면:** 학습 속도가 매우 느려집니다.
- 적절한 학습률을 설정하여, 여러 데이터의 특성을 점진적으로 반영하며 안정적으로 최적의 매개변수를 찾아가는 것이 중요합니다.

## 4. 로지스틱 회귀 분석: 분류 문제 해결

로지스틱 회귀는 결과가 '합격/불합격', '참/거짓'처럼 두 개의 범주로 나뉘는 이진 분류(Binary Classification) 문제에 사용되는 핵심 알고리즘입니다.

### 정의 및 특징

- **목표:** 데이터를 0과 1 사이의 확률 값으로 예측하여 특정 범주에 속할 확률을 나타내는 것.
- **결정 경계:** 선형 회귀처럼 직선을 사용하는 대신, 0과 1 사이를 구분하는 **S자 형태의 곡선**을 그립니다.

### 시그모이드 함수 (Sigmoid Function)

이 S자 형태의 곡선을 생성하는 것이 바로 시그모이드 함수이며, 로지스틱 회귀에서는 **활성화 함수(Activation Function)**로 사용됩니다.

- **공식:** `y = 1 / (1 + e^-(ax+b))`
- **역할:** 입력값(`ax+b`)을 받아 `0`에서 `1` 사이의 확률 값으로 변환합니다. 입력값이 매우 크면 1에 가까워지고, 매우 작으면 0에 가까워집니다.
- **매개변수의 영향:**
    - `a` 값이 커지면 S자 곡선의 경사가 가파르게 변합니다.
    - `b` 값에 따라 곡선이 좌우로 이동합니다.

### 학습 과정

- **모델 (가설):** 입력 데이터의 패턴을 수학적 표현(`wᵀx + b`)으로 설정합니다.
- **활성화 함수:** 모델의 출력값을 시그모이드 함수에 통과시켜 0~1 사이의 확률 예측값(`ŷ`)을 얻습니다.
- **손실 함수 (Loss Function):** 예측값(`ŷ`)과 실제값(`y`)의 차이를 측정하는 척도입니다. 로지스틱 회귀에서는 주로 **크로스 엔트로피(Cross Entropy)** 함수를 사용하여 두 확률 분포 간의 차이를 최소화하는 방향으로 모델을 학습시킵니다.
- **구조:** 로지스틱 회귀는 선형 함수와 비선형 함수(시그모이드)가 결합된 단일 레이어(Layer) 구조로 볼 수 있으며, 이는 신경망의 기본 구성 요소와 유사합니다.