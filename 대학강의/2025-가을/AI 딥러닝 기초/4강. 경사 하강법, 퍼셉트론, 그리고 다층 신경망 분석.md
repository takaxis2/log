

## 요약

본 문서는 인공지능 및 딥러닝의 핵심 개념인 경사 하강법, 퍼셉트론, 다층 퍼셉트론에 대한 분석을 제공한다. 기계가 학습하는 과정의 근본에는 모델의 오차를 최소화하는 최적화 원리가 있으며, **경사 하강법(Gradient Descent)**은 이 과정을 수행하는 핵심 알고리즘이다. 이 방법은 오차 함수의 기울기(미분값)를 계산하고, 기울기의 반대 방향으로 모델의 매개변수를 반복적으로 조정하여 오차를 최소화한다. 이때 **학습률(learning rate)**의 설정은 수렴 속도와 안정성을 결정하는 매우 중요한 요소로, 너무 클 경우 최적값을 찾지 못하고 발산할 위험이 있다.

인공신경망의 초기 모델인 **퍼셉트론(Perceptron)**은 생물학적 뉴런에서 영감을 받아 개발되었으며, 입력값에 가중치를 곱하고 활성화 함수를 통해 출력을 결정하는 단순한 구조를 가진다. 그러나 퍼셉트론은 단일 직선으로 데이터를 구분하는 선형 분리만 가능하여, **XOR 문제**와 같은 비선형 문제는 해결할 수 없다는 치명적인 한계를 드러냈다. 1969년 마빈 민스키(Marvin Minsky)가 이 문제를 공식적으로 제기하며 인공지능 연구는 한동안 침체기를 겪었다.

이 한계를 극복한 것이 바로 **다층 퍼셉트론(Multi-Layer Perceptron, MLP)**이다. 입력층과 출력층 사이에 하나 이상의 **은닉층(hidden layer)**을 도입함으로써, MLP는 평면을 왜곡하거나 더 높은 차원에서 문제를 바라보는 것과 같은 효과를 내어 복잡한 비선형 관계를 학습할 수 있게 되었다. 은닉층은 단순한 선형 함수의 조합을 통해 비선형 결정 경계를 생성하여 XOR 문제와 같은 난제를 해결하는 열쇠가 되었다. 이러한 다층 구조의 신경망, 즉 **심층 신경망(Deep Neural Network, DNN)**을 학습시키는 과정이 바로 **딥러닝(Deep Learning)**이며, 현대 인공지능 기술의 근간을 이룬다.

--------------------------------------------------------------------------------

## I. 경사 하강법: 머신러닝 최적화의 핵심 원리

경사 하강법은 딥러닝을 포함한 머신러닝 모델을 학습시키는 데 사용되는 가장 중요한 최적화 알고리즘이다. 이 알고리즘의 목표는 모델의 예측값과 실제값 사이의 오차를 최소화하는 최적의 매개변수(가중치와 바이어스)를 찾는 것이다.

### 1. 기울기와 오차의 관계

선형 회귀 분석에서 모델의 성능은 오차(error)로 측정된다. 기울기(`a`)와 y절편(`b`) 같은 매개변수를 어떻게 설정하느냐에 따라 오차의 크기가 달라진다.

- **오차 함수**: 기울기와 오차 사이의 관계는 일반적으로 아래로 볼록한 이차 함수(포물선) 형태를 띤다. 즉, 최적의 기울기 값에서 멀어질수록(너무 크거나 너무 작아질수록) 오차는 기하급수적으로 커진다.
- **최적점**: 이 이차 함수 그래프의 가장 낮은 지점, 즉 꼭짓점이 오차가 최소가 되는 지점이며, 이때의 기울기 값이 우리가 찾고자 하는 최적의 매개변수 값(`m`)이다.

### 2. 미분을 통한 최솟값 탐색

딥러닝을 이해하는 데 가장 중요한 수학적 원리는 미분이다. 미분은 특정 지점에서의 '순간 변화율' 또는 '기울기'를 구하는 과정이다.

- **순간 변화율**: 그래프의 한 점에서의 미분 계수는 그 점에서의 접선의 기울기와 같다.
- **최솟값의 조건**: 오차 함수의 최솟값은 그래프의 꼭짓점에 해당하며, 이 지점에서는 접선의 기울기가 0이 된다(x축과 평행).
- **미분의 활용**: 따라서 오차 함수를 미분하여 그 값이 0이 되는 지점을 찾으면, 이는 오차를 최소화하는 매개변수 값을 찾는 것과 동일하다.

### 3. 경사 하강법 알고리즘과 학습률

경사 하강법은 미분을 이용해 오차가 최소가 되는 지점을 반복적으로 찾아 나가는 방법이다.

- **알고리즘 절차**:
    1. 임의의 매개변수 값(예: `a1`)에서 시작하여 해당 지점의 기울기(미분값)를 계산한다.
    2. 계산된 기울기의 반대 방향으로 매개변수 값을 소폭 이동시킨다. (기울기가 양수이면 음의 방향으로, 음수이면 양의 방향으로 이동)
    3. 새로운 지점(`a2`)에서 다시 기울기를 계산하고, 이 과정을 반복한다.
    4. 기울기가 0에 가까워지면 반복을 중단한다. 이 지점이 바로 오차가 최소화된 최적의 매개변수 값이다.
- **학습률 (Learning Rate)**:
    - **정의**: 경사 하강법에서 매개변수를 업데이트할 때 한 번에 얼마나 이동할지, 즉 이동 거리를 결정하는 값이다.
    - **중요성**: 학습률은 모델의 수렴에 결정적인 영향을 미친다.
        - **학습률이 너무 큰 경우**: 최솟값을 지나쳐 반대편으로 이동하는 현상이 반복되면서 값이 한 점으로 수렴하지 못하고 **발산(divergence)**할 수 있다.
        - **학습률이 너무 작은 경우**: 수렴 속도가 매우 느려져 학습에 오랜 시간이 걸릴 수 있다.
    - **최적화**: 딥러닝에서는 최적의 학습률을 찾아 모델의 성능을 극대화하는 것이 중요한 최적화 과정 중 하나이다.

## II. 퍼셉트론: 인공신경망의 기원과 한계

퍼셉트론은 인공신경망의 가장 기본적인 단위로, 인간의 뇌를 구성하는 뉴런의 작동 방식에서 영감을 얻어 개발되었다.

### 1. 개념과 역사

- **생물학적 영감**: 인간의 뇌는 뉴런들이 시냅스를 통해 연결된 거대한 네트워크다. 뉴런은 특정 임계값(threshold) 이상의 자극을 받으면 다음 뉴런으로 신호를 전달한다.
- **인공신경망의 시작**:
    - 1943년, 맥컬럭-월터 피츠(McCulloch-Walter Pitts)는 '켜고 끄는 기능이 있는 신경'을 그물망처럼 연결하면 뇌처럼 동작할 수 있다는 개념을 처음 제시했다.
    - 1957년, 신경 생물학자 프랑크 로젠블랫(Frank Rosenblatt)은 이 개념을 실제 장치인 **퍼셉트론(Perceptron)**으로 구현했다.
- **작동 원리**: 퍼셉트론은 여러 입력값(`X1`, `X2`, ...)에 각각의 가중치(`W1`, `W2`, ...)를 곱한 후, 바이어스(`b`)를 더한 **가중합(weighted sum)**을 계산한다. 이 값을 활성화 함수에 통과시켜 최종 출력값을 결정한다. 이 가중치를 조절하는 과정을 통해 최초로 '학습'의 개념을 도입했다.

### 2. 단층 퍼셉트론의 근본적 한계: XOR 문제

초기 퍼셉트론 연구는 인공지능의 밝은 미래를 예고하는 듯했으나, 곧 명확한 한계에 부딪혔다.

- **선형 분리의 한계**: 단층 퍼셉트론이나 이를 개선한 아달라인(Adaline)은 2차원 평면상에서 데이터를 하나의 직선으로만 분리할 수 있다. 즉, **선형적으로 분리 가능한(linearly separable)** 문제만 해결할 수 있다.
- **XOR 문제**:
    - 컴퓨터의 기본 논리 회로인 AND, OR 게이트는 입력값을 좌표 평면에 표시했을 때 하나의 직선으로 참(1)과 거짓(0)을 구분할 수 있다.
    - 하지만 XOR(배타적 논리합) 게이트는 입력값 두 개가 서로 다를 때만 참(1)을 출력한다. 이를 좌표 평면에 나타내면 어떤 단일 직선으로도 참과 거짓을 완벽하게 나눌 수 없다.
    - 이는 단층 퍼셉트론이 해결할 수 없는 대표적인 **비선형(non-linear) 문제**이다.
- **AI 연구의 침체기**: 1969년, MIT의 마빈 민스키 교수는 논문 "퍼셉트론즈(Perceptrons)"를 통해 이 한계를 수학적으로 증명했다. 이 논문은 인공지능 연구에 대한 기대를 크게 꺾었고, 이후 연구는 오랜 침체기를 겪게 되었다.

## III. 다층 퍼셉트론: 비선형 문제 해결과 딥러닝의 서막

단층 퍼셉트론의 한계는 새로운 차원의 접근법을 통해 극복되었다. 2차원 평면에서 해결할 수 없는 문제를 3차원 공간에서 해결하는 것처럼, 인공신경망의 차원을 달리하는 아이디어가 등장했다.

### 1. 은닉층의 도입

- **문제 해결의 발상**: XOR 문제를 해결하는 핵심 아이디어는 '평면을 휘어주는 것'이다. 이는 신경망 구조상에서 여러 개의 퍼셉트론을 동시에 계산하는 방식으로 구현된다.
- **은닉층 (Hidden Layer)**: 이 아이디어를 실현하기 위해 입력층과 출력층 사이에 중간 단계의 층인 **은닉층**을 추가했다. 은닉층이 포함된 신경망을 **다층 퍼셉트론(Multi-Layer Perceptron, MLP)**이라고 한다.
- **비선형성 확보**: 은닉층은 입력 데이터를 내부적으로 다른 공간으로 변환하는 역할을 한다. 이를 통해 선형적으로 분리할 수 없었던 데이터를 분리 가능한 형태로 만들어, 결과적으로 비선형 문제를 해결할 수 있게 된다.

### 2. 구조와 작동 원리

MLP는 일반적으로 세 가지 종류의 층으로 구성된다.

- **입력층 (Input Layer)**: 외부에서 데이터를 받아들이는 층.
- **은닉층 (Hidden Layer)**: 입력층과 출력층 사이에 위치하며, 입력 데이터의 특징을 추출하고 조합하는 역할을 한다. 은닉층의 각 노드는 입력층의 모든 노드와 연결되어 가중합을 계산하고, 시그모이드(sigmoid) 같은 활성화 함수를 통해 다음 층으로 값을 전달한다.
- **출력층 (Output Layer)**: 은닉층에서 처리된 값을 받아 최종 결과값을 출력하는 층.

### 3. XOR 문제 해결 메커니즘

다층 퍼셉트론은 여러 논리 게이트를 조합하여 XOR 문제를 해결한다. 예를 들어, 다음과 같은 구조로 XOR를 구현할 수 있다.

1. **은닉층**: 입력값 `x1`, `x2`를 받아 첫 번째 노드에서는 NAND 연산을, 두 번째 노드에서는 OR 연산을 동시에 수행한다.
2. **출력층**: 은닉층의 두 노드에서 나온 결과값(NAND 결과, OR 결과)을 입력으로 받아 AND 연산을 수행한다.
3. **결과**: 이 조합을 통해 최종적으로 XOR 연산과 동일한 결과를 출력할 수 있다. 숨겨진 두 개의 노드(은닉층)를 둔 다층 구조 덕분에 XOR 문제가 해결되는 것이다.

|   |   |   |   |   |   |
|---|---|---|---|---|---|
|x1|x2|n1 (NAND)|n2 (OR)|yout (AND)|XOR 목표값|
|0|0|≈ 1|≈ 0|≈ 0|**0**|
|0|1|≈ 1|≈ 1|≈ 1|**1**|
|1|0|≈ 1|≈ 1|≈ 1|**1**|
|1|1|≈ 0|≈ 1|≈ 0|**0**|

_※ 적절한 가중치와 바이어스 값을 설정했을 때의 계산 결과 예시_

### 4. 심층 신경망으로의 확장

- **심층 신경망 (Deep Neural Network, DNN)**: 은닉층이 2개 이상인 다층 퍼셉트론을 심층 신경망이라고 부른다. 층이 깊어질수록 더 복잡하고 추상적인 특징을 학습할 수 있다.
- **딥러닝 (Deep Learning)**: 심층 신경망을 이용해 기계가 스스로 최적의 가중치를 찾아내도록 자동화시키는 학습 과정을 딥러닝이라고 한다. 다층 퍼셉트론과 XOR 문제의 해결은 현대 딥러닝 시대의 서막을 연 중요한 돌파구였다.