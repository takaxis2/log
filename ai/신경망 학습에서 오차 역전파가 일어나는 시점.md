오차 역전파(**Backpropagation**)는 신경망 학습 과정에서 **순전파(Forward Propagation)**가 완료된 후, **손실 함수(Loss Function)의 오차**가 계산되었을 때 일어납니다.

이 과정은 계산된 오차를 줄이기 위해 신경망의 **가중치(Weights)와 편향(Biases)**을 갱신하는 데 사용됩니다. 구체적으로는 다음과 같이 진행됩니다.

---

## 1. 오차 역전파가 일어나는 시점

오차 역전파는 **신경망 학습의 한 단계**로서, 보통 다음과 같은 순서로 진행되는 **경사 하강법(Gradient Descent)** 기반의 최적화 과정의 일부입니다.

1. **순전파 (Forward Propagation):** 입력 데이터가 신경망의 입력층에서 출력층까지 전달되어 예측값($Y_{\text{out}}$)을 계산합니다.
    
2. **손실 계산 (Loss Calculation):** 예측값과 실제 정답($Y_{\text{true}}$) 사이의 **오차(Error) 또는 손실(Loss)**을 손실 함수를 사용하여 계산합니다.
    
3. **역전파 (Backpropagation):**
    
    - 계산된 오차를 출력층에서부터 입력층 방향으로 **역방향**으로 전파하며, **연쇄 법칙(Chain Rule)**을 사용하여 각 가중치 및 편향에 대한 손실 함수의 **기울기(Gradient)**를 효율적으로 계산합니다.
        
4. **매개변수 갱신 (Parameter Update):** 계산된 기울기를 바탕으로 **옵티마이저(Optimizer)**(예: SGD, Adam)를 사용하여 가중치와 편향을 오차가 감소하는 방향으로 아주 조금씩 갱신합니다.
    

이 1~4단계를 **반복(Iteration)하거나 전체 데이터셋에 대해 반복(Epoch)**하면서 신경망을 학습시킵니다. 따라서 **순전파가 끝나고 손실이 계산된 직후**가 역전파의 시작 시점입니다.

---

## 2. 코드에서의 선언 위치

딥러닝 프레임워크(예: PyTorch, TensorFlow/Keras)를 사용할 경우, 오차 역전파 과정은 보통 **명시적으로 선언되지 않고** 프레임워크 내부에서 **자동으로 처리**됩니다.

주요 프레임워크에서 역전파가 실행되는 코드는 주로 **옵티마이저(Optimizer)**와 관련된 함수 호출에서 발생합니다.

### PyTorch (파이토치)의 경우

PyTorch는 **자동 미분(Autograd)** 시스템을 사용하여 역전파를 처리합니다.

Python

```
# 1. 순전파 및 손실 계산
outputs = model(inputs)
loss = criterion(outputs, labels)

# 2. 기울기 초기화 (이전 배치의 기울기 삭제)
optimizer.zero_grad() 

# 3. ⭐️ 오차 역전파 실행 (기울기 계산) ⭐️
loss.backward() 

# 4. 가중치 갱신
optimizer.step()
```

- `loss.backward()`: 이 함수를 호출하는 순간, 손실(Loss)로부터 역방향으로 이동하며 모델의 모든 학습 가능한 매개변수(**가중치와 편향**)에 대한 기울기($\nabla W, \nabla B$)가 자동으로 계산됩니다.
    

### TensorFlow/Keras (텐서플로우/케라스)의 경우

Keras와 같은 고수준 API에서는 일반적으로 **`model.fit()`** 함수 내부에 순전파, 손실 계산, 역전파, 가중치 갱신이 모두 통합되어 있습니다.

Python

```
# 모델 컴파일 시 손실 함수와 옵티마이저 지정
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# ⭐️ 학습 실행 (내부에 순전파 및 역전파 과정 포함) ⭐️
model.fit(train_data, train_labels, epochs=10, batch_size=32)
```

- `model.fit()`: 이 함수가 호출되면 내부적으로 **각 배치(Batch)마다 순전파와 손실 계산 후, 옵티마이저가 역전파를 사용하여 기울기를 계산하고 가중치를 갱신**하는 일련의 과정을 반복합니다.
    
- **저수준(Low-level)** 코드로 직접 학습 루프를 작성할 경우, PyTorch의 `loss.backward()`와 유사하게 **`tf.GradientTape`**를 사용하여 기울기를 기록하고 계산합니다.