**오차 역전파(Backpropagation)**는 **가중치 업데이트** 그 자체가 아니라, 가중치를 업데이트하는 데 **필요한 정보(기울기)**를 효율적으로 계산하는 알고리즘입니다.

모델 학습 과정에서 역전파와 가중치 업데이트의 역할은 다음과 같이 구분할 수 있습니다.

---

## 1. 오차 역전파의 역할: 기울기 계산 

오차 역전파의 핵심 역할은 **기울기(Gradient)**를 구하는 것입니다.

- **기울기(Gradient):** 손실 함수 값(오차)을 최소화하기 위해 각 가중치($W$)와 편향($B$)이 **얼마나, 어떤 방향으로** 변화해야 하는지를 나타내는 값입니다. 즉, $$
\frac{\partial \text{Loss}}{\partial W}
$$
를 계산합니다.
    
- 역전파는 **연쇄 법칙(Chain Rule)**을 사용하여 이 기울기를 출력층부터 입력층까지 역으로 전파하며 매우 효율적으로 계산합니다.
    

## 2. 가중치 업데이트의 역할: 매개변수 갱신 🛠️

가중치 업데이트는 **옵티마이저(Optimizer)**에 의해 수행됩니다.

- **옵티마이저(Optimizer):** 역전파로 계산된 **기울기**를 입력받아, 이를 학습률(Learning Rate) 등의 하이퍼파라미터와 결합하여 실제로 가중치 값을 **갱신**하는 함수 또는 알고리즘입니다 (예: 경사 하강법, Adam).
    
- **갱신 공식 (예시):** $\text{새로운 } W = \text{현재 } W - (\text{학습률} \times \text{기울기})$
    

### 비유를 통한 정리

산을 내려가는 상황에 비유하면 이해하기 쉽습니다.

|**요소**|**신경망 학습 과정**|**역할**|
|---|---|---|
|**산**|손실 함수 (Loss Function)|내려가야 할 지점 (최소 오차)|
|**현재 위치**|현재 가중치($W$)|현재 위치|
|**오차 역전파**|기울기 계산|"지금 서 있는 곳에서 경사가 **가장 가파른 방향**은 어디인가?"를 알려줌|
|**옵티마이저**|가중치 업데이트|"가장 가파른 방향으로 **얼마만큼(학습률)** 발을 옮길 것인가?"를 결정하고 실제로 이동|

따라서 **역전파는 '방향을 알려주는 계산'**이고, **옵티마이저는 '실제로 움직여 값을 바꾸는 함수'**라고 정리할 수 있습니다.