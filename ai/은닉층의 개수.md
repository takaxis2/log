신경망에 은닉층이 많아지면(즉, **깊어지면**) 모델의 **표현 능력(Representation Power)**이 향상되어 학습 효과가 좋아질 수 있습니다. 하지만 항상 그런 것은 아니며, 단점과 해결책도 함께 고려해야 합니다.

---

## 은닉층 증가의 장점 (학습 효과 향상)

은닉층을 여러 개 쌓은 신경망을 **딥러닝(Deep Learning)** 모델이라고 부르며, 다음과 같은 장점으로 인해 학습 효과가 좋아집니다.

1. **계층적 특징 학습 (Hierarchical Feature Learning) :**
    
    - 깊은 신경망은 데이터를 **추상적인 수준으로 단계적으로** 이해할 수 있습니다.
        
    - **초기 은닉층**은 단순하고 일반적인 특징(예: 이미지의 경우 에지, 선, 색상)을 학습합니다.
        
    - **후기 은닉층**은 앞선 층에서 학습한 단순한 특징들을 결합하여 복잡하고 추상적인 특징(예: 이미지의 경우 눈, 코, 귀, 전체 얼굴)을 학습합니다.
        
    - 이러한 계층적 구조 덕분에 복잡한 문제의 본질을 더 잘 파악할 수 있습니다.
        
2. **비선형성 증가 및 표현력 향상 :**
    
    - 각 은닉층에는 활성화 함수(Activation Function)가 적용되어 **비선형성**이 추가됩니다.
        
    - 층이 많을수록 모델이 학습할 수 있는 **비선형 함수의 종류와 복잡도**가 기하급수적으로 늘어나며, 이는 데이터에 내재된 복잡한 관계를 더 정확하게 모델링할 수 있게 해줍니다.
        

---

## 은닉층 증가의 단점 및 한계

은닉층을 무작정 늘린다고 해서 항상 좋은 결과가 나오는 것은 아니며, 다음과 같은 문제들이 발생할 수 있습니다.

1. **기울기 소실 (Vanishing Gradient) 또는 폭주 (Exploding Gradient) 문제 :**
    
    - 오차 역전파 과정에서 오차 신호가 깊은 층을 통과하며 점차 **너무 작아지거나(소실)** **너무 커지는(폭주)** 현상이 발생할 수 있습니다.
        
    - 기울기가 소실되면, 네트워크의 초기 층(입력층에 가까운 층)에 학습 신호가 제대로 전달되지 않아 **가중치 업데이트가 거의 이루어지지 않습니다.**
        
2. **과적합 (Overfitting) 위험 증가 :**
    
    - 모델의 매개변수(가중치와 편향) 수가 많아지면서, 모델이 훈련 데이터의 **노이즈나 사소한 특성까지 암기**하게 되어 일반화 성능(새로운 데이터에 대한 예측 성능)이 떨어질 수 있습니다.
        
3. **계산 비용 및 시간 증가 :**
    
    - 모델의 크기가 커지면 훈련 및 예측에 필요한 **계산 자원과 시간**이 크게 늘어납니다.
        

---

## 해결책 (깊은 신경망 학습 기법)

현재의 딥러닝은 위와 같은 단점들을 극복하고 깊은 신경망의 장점을 활용하기 위해 다양한 기술을 사용합니다.

- **잔차 연결 (Residual Connections):** ResNet과 같은 모델에서 사용되며, 기울기 소실 문제를 완화하고 깊은 네트워크의 학습을 용이하게 합니다.
    
- **배치 정규화 (Batch Normalization):** 학습 과정에서 각 층의 입력 분포를 안정화시켜 기울기 문제를 완화하고 학습 속도를 높입니다.
    
- **드롭아웃 (Dropout):** 훈련 중 일부 뉴런을 무작위로 비활성화하여 과적합을 방지하고 일반화 성능을 높입니다.
    
- **ReLU 계열 활성화 함수:** Sigmoid나 Tanh 대신 ReLU(Rectified Linear Unit) 계열 함수를 사용하여 계산 비용을 줄이고 기울기 소실 문제를 개선합니다.